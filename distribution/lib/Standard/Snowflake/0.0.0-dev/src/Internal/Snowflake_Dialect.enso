private

from Standard.Base import all
import Standard.Base.Errors.Illegal_Argument.Illegal_Argument
import Standard.Base.Errors.Illegal_State.Illegal_State
import Standard.Base.Errors.Unimplemented.Unimplemented
import Standard.Base.Runtime.Ref.Ref

import Standard.Table.Internal.Problem_Builder.Problem_Builder
import Standard.Table.Internal.Vector_Builder.Vector_Builder
from Standard.Table import Aggregate_Column, Column, Value_Type
from Standard.Table.Aggregate_Column.Aggregate_Column import all
from Standard.Table.Errors import Inexact_Type_Coercion
from Standard.Table.Internal.Storage import get_storage_for_column

import Standard.Database.Connection.Connection.Connection
import Standard.Database.DB_Column.DB_Column
import Standard.Database.DB_Table.DB_Table
import Standard.Database.Dialect
import Standard.Database.Feature.Feature
import Standard.Database.Internal.Aggregate_Helper
import Standard.Database.Internal.Aggregate_Helper.Aggregate_With_Helper_Expressions
import Standard.Database.Internal.Base_Generator
import Standard.Database.Internal.Common.Database_Distinct_Helper
import Standard.Database.Internal.Common.Database_Join_Helper
import Standard.Database.Internal.Common.Row_Number_Helpers
import Standard.Database.Internal.Error_Mapper.Error_Mapper
import Standard.Database.Internal.Internals_Access
import Standard.Database.Internal.IR.Context.Context
import Standard.Database.Internal.IR.Context.Context_Extension
import Standard.Database.Internal.IR.From_Spec.From_Spec
import Standard.Database.Internal.IR.Internal_Column.Internal_Column
import Standard.Database.Internal.IR.Nulls_Order.Nulls_Order
import Standard.Database.Internal.IR.Order_Descriptor.Order_Descriptor
import Standard.Database.Internal.IR.Query.Query
import Standard.Database.Internal.IR.SQL_Expression.SQL_Expression
import Standard.Database.Internal.IR.SQL_Join_Kind.SQL_Join_Kind
import Standard.Database.Internal.Replace_Params.Replace_Params
import Standard.Database.Internal.SQL_Type_Mapping.SQL_Type_Mapping
import Standard.Database.Internal.SQL_Type_Reference.SQL_Type_Reference
import Standard.Database.Internal.Statement_Setter.Statement_Setter
import Standard.Database.Dialect_Flag.Dialect_Flag
import Standard.Database.SQL.SQL_Builder
import Standard.Database.SQL.SQL_Fragment
import Standard.Database.SQL_Statement.SQL_Statement
import Standard.Database.SQL_Type.SQL_Type
from Standard.Database.Dialect import Temp_Table_Style
from Standard.Database.Errors import SQL_Error, Unsupported_Database_Operation
from Standard.Database.Internal.IR.Operation_Metadata import Date_Period_Metadata
from Standard.Database.Internal.JDBC_Connection import JDBC_Connection
from Standard.Database.Internal.Statement_Setter import fill_hole_default

import project.Internal.Snowflake_Error_Mapper.Snowflake_Error_Mapper
import project.Internal.Snowflake_Type_Mapping.Snowflake_Type_Mapping

polyglot java import org.enso.database.JDBCUtils
polyglot java import org.enso.snowflake.SnowflakeJDBCUtils

## PRIVATE
   The dialect of Snowflake databases.
snowflake : Snowflake_Dialect
snowflake =
    Snowflake_Dialect.Value make_dialect_operations

## PRIVATE
   The dialect of Snowflake databases.
type Snowflake_Dialect
    ## PRIVATE
       The dialect of Snowflake databases.
    Value dialect_operations

    ## PRIVATE
       Name of the dialect.
    name : Text
    name self = snowflake_dialect_name

    ## PRIVATE
    to_text : Text
    to_text self = "Snowflake_Dialect"

    ## PRIVATE
       A function which generates SQL code from the internal representation
       according to the specific dialect.
    generate_sql : Query -> SQL_Statement
    generate_sql self query =
        Base_Generator.generate_query self query . build

    ## PRIVATE
       Generates SQL to truncate a table.
    generate_truncate_table_sql : Text -> SQL_Builder
    generate_truncate_table_sql self table_name =
        Base_Generator.truncate_table_truncate_table_style self table_name

    ## PRIVATE
       Generates SQL modifier for limiting the number of rows and its position in the query
    get_limit_sql_modifier : Integer -> Any
    get_limit_sql_modifier self limit =
        [700, SQL_Builder.code (" LIMIT " + limit.to_text)]

    ## PRIVATE
       Wraps and possibly escapes the identifier so that it can be used in a
       generated query regardless of what characters it contains.
       The quotes used will depend on the dialect.
    wrap_identifier : Text -> SQL_Builder
    wrap_identifier self identifier =
        Base_Generator.wrap_in_quotes identifier

    ## PRIVATE
       Generates a SQL expression for a table literal.
    make_table_literal : Vector (Vector Text) -> Vector Text -> Text -> SQL_Builder
    make_table_literal self vecs column_names as_name =
        Base_Generator.default_make_table_literal self.wrap_identifier vecs column_names as_name

    ## PRIVATE
       Prepares an ordering descriptor.

       One of the purposes of this method is to verify if the expected ordering
       settings are supported by the given database backend.

       Arguments:
       - internal_column: the column to order by.
       - sort_direction: the direction of the ordering.
       - text_ordering: If provided, specifies that the column should be treated
         as text values according to the provided ordering. For non-text types,
         it should be set to `Nothing`.
    prepare_order_descriptor : Internal_Column -> Sort_Direction -> Nothing | Text_Ordering -> Order_Descriptor
    prepare_order_descriptor self internal_column sort_direction text_ordering =
        make_order_descriptor internal_column sort_direction text_ordering

    ## PRIVATE
       Prepares a distinct operation.
    prepare_distinct : DB_Table -> Vector -> Case_Sensitivity -> Problem_Builder -> DB_Table
    prepare_distinct self table key_columns case_sensitivity problem_builder =
        table_name_deduplicator = (Internals_Access.get_connection table).base_connection.table_naming_helper.create_unique_name_strategy
        table_name_deduplicator.mark_used table.name
        inner_table_alias = table_name_deduplicator.make_unique table.name+"_inner"
        setup = (Internals_Access.get_context table).as_subquery inner_table_alias [Internals_Access.internal_columns table]
        new_columns = setup.new_columns.first
        column_mapping = Dictionary.from_vector <| new_columns.map c-> [c.name, c]
        new_key_columns = key_columns.map c-> column_mapping.at c.name
        type_mapping = self.get_type_mapping
        # TODO simpler distinct if distinct_expressions are all columns?
        distinct_expressions = new_key_columns.map column->
            value_type = type_mapping.sql_type_to_value_type column.sql_type_reference.get
            Database_Distinct_Helper.make_distinct_expression case_sensitivity problem_builder column value_type
        new_context = Context.for_subquery setup.subquery
            . add_extension (make_distinct_extension distinct_expressions)
        table.updated_context_and_columns new_context new_columns subquery=True

    ## PRIVATE
       Returns the mapping between SQL types of this dialect and Enso
       `Value_Type`.
    get_type_mapping : SQL_Type_Mapping
    get_type_mapping self = Snowflake_Type_Mapping

    ## PRIVATE
    get_statement_setter : Statement_Setter
    get_statement_setter self =
        custom_fill_hole stmt i type_hint value = case value of
            _ : Date_Time ->
                keep_offset = case type_hint of
                    Value_Type.Date_Time with_timezone -> with_timezone
                    _ -> True
                SnowflakeJDBCUtils.setDateTime stmt i value keep_offset
            _ : Time_Of_Day -> SnowflakeJDBCUtils.setTimeOfDay stmt i value
            _ : Date        -> SnowflakeJDBCUtils.setDate stmt i value
            # Fallback to default logic for everything else
            _ -> fill_hole_default stmt i type_hint value
        Statement_Setter.Value custom_fill_hole

    ## PRIVATE
    make_cast : Internal_Column -> SQL_Type -> (SQL_Expression -> SQL_Type_Reference) -> Internal_Column
    make_cast self column target_type infer_result_type_from_database_callback =
        mapping = self.get_type_mapping
        source_type = mapping.sql_type_to_value_type column.sql_type_reference.get
        target_value_type = mapping.sql_type_to_value_type target_type

        new_expression = make_custom_cast column source_type target_value_type . if_nothing <|
            source_expression = Internals_Access.column_expression column
            target_type_sql_text = mapping.sql_type_to_text target_type
            SQL_Expression.Operation "CAST" [source_expression, SQL_Expression.Literal target_type_sql_text]

        new_sql_type_reference = infer_result_type_from_database_callback new_expression
        Internal_Column.Value column.name new_sql_type_reference new_expression

    ## PRIVATE
    needs_execute_query_for_type_inference : Text | SQL_Statement -> Boolean
    needs_execute_query_for_type_inference self statement =
        query_text = case statement of
            text : Text -> text
            _ : SQL_Statement -> statement.prepare.first
        keywords_that_need_execute = ["VALUES", "DECODE"]
        regex = keywords_that_need_execute.join "|"
        needs_execute = query_text.find regex . is_nothing . not
        needs_execute

    ## PRIVATE
       Specifies how the database creates temp tables.
    temp_table_style : Temp_Table_Style
    temp_table_style self = Temp_Table_Style.Temporary_Table

    ## PRIVATE
    adapt_unified_column : Internal_Column -> Value_Type -> (SQL_Expression -> SQL_Type_Reference) -> Internal_Column
    adapt_unified_column self column approximate_result_type infer_result_type_from_database_callback =
        _ = [approximate_result_type, infer_result_type_from_database_callback]
        column

    ## PRIVATE
       Add an extra cast to adjust the output type of certain operations with
       certain arguments.

       It is used when the normal type inference provided by the database engine
       needs to be adjusted.

       In most cases this method will just return the expression unchanged, it
       is used only to override the type in cases where the default one that the
       database uses is not what we want.
    cast_op_type self (op_kind:Text) (args:(Vector Internal_Column)) (expression:SQL_Expression) =
        _ = [op_kind, args]
        expression

    ## PRIVATE
    prepare_fetch_types_query : SQL_Expression -> Context -> SQL_Statement
    prepare_fetch_types_query self expression context =
        Base_Generator.default_fetch_types_query self expression context

    ## PRIVATE
    check_aggregate_support : Aggregate_Column -> Boolean ! Unsupported_Database_Operation
    check_aggregate_support self aggregate =
        # All aggregates are currently supported.
        _ = aggregate
        True

    ## PRIVATE
       Checks if an operation is supported by the dialect.
    is_operation_supported self operation:Text -> Boolean =
        self.dialect_operations.is_operation_supported operation

    ## PRIVATE
       Checks if a feature is supported by the dialect.
    is_feature_supported self feature:Feature -> Boolean =
        _ = feature
        True

    ## PRIVATE
       Checks a dialect flag
    flagged self flag:Dialect_Flag -> Boolean =
        case flag of
            Dialect_Flag.Supports_Negative_Decimal_Places -> True
            Dialect_Flag.Supports_Float_Decimal_Places -> True
            Dialect_Flag.Use_Builtin_Bankers -> True
            Dialect_Flag.Primary_Key_Allows_Nulls -> False
            Dialect_Flag.Supports_Separate_NaN -> True
            Dialect_Flag.Supports_Nested_With_Clause -> True
            Dialect_Flag.Supports_Case_Sensitive_Columns -> True

    ## PRIVATE
       The default table types to use when listing tables.
    default_table_types : Vector Text
    default_table_types self =
        ["TABLE", "VIEW", "TEMPORARY TABLE", "TEMPORARY VIEW", "MATERIALIZED VIEW"]

    ## PRIVATE
    get_error_mapper : Error_Mapper
    get_error_mapper self = Snowflake_Error_Mapper

    ## PRIVATE
       The dialect-dependent strategy to get the Primary Key for a given table.

       Returns `Nothing` if the key is not defined.
    fetch_primary_key : Connection -> Text -> Vector Text ! Nothing
    fetch_primary_key self connection table_name =
        Dialect.default_fetch_primary_key connection table_name

    ## PRIVATE
       Prepares metadata for an operation taking a date/time period and checks
       if the given period is supported.
    prepare_metadata_for_period : Date_Period | Time_Period -> Value_Type -> Any
    prepare_metadata_for_period self period operation_input_type =
        Date_Period_Metadata.Value period operation_input_type

    ## PRIVATE
       Returns true if the `replace` parameters are supported by this backend.
    if_replace_params_supports : Replace_Params -> Any -> Any
    if_replace_params_supports self replace_params ~action =
        if supported_replace_params.contains replace_params then action else replace_params.throw_unsupported snowflake_dialect_name

    ## PRIVATE
    value_type_for_upload_of_existing_column : DB_Column -> Value_Type
    value_type_for_upload_of_existing_column self column = case column of
        # Return the type as-is for database columns.
        _ : DB_Column -> column.value_type
        _ : Column ->
            base_type = column.value_type
            case base_type of
                Value_Type.Decimal precision scale ->
                    used_scale = scale.if_nothing 12
                    used_precision = Math.min 38 (precision.if_nothing 38)
                    new_type = Value_Type.Decimal used_precision used_scale
                    if used_scale==scale && used_precision==precision then new_type else
                        Warning.attach (Inexact_Type_Coercion.Warning base_type new_type unavailable=False) new_type
                _ -> base_type

    ## PRIVATE
    needs_literal_table_cast : Value_Type -> Boolean
    needs_literal_table_cast self value_type = case value_type of
        Value_Type.Time        -> True
        Value_Type.Date_Time _ -> True
        _                      -> False

    ## PRIVATE
    generate_column_for_select self base_gen expr:(SQL_Expression | Order_Descriptor | Query) name:Text -> SQL_Builder =
        base_gen.default_generate_column self expr name

    ## PRIVATE
    ensure_query_has_no_holes : JDBC_Connection -> Text -> Nothing ! Illegal_Argument
    ensure_query_has_no_holes self jdbc:JDBC_Connection raw_sql:Text =
        jdbc.ensure_query_has_no_holes raw_sql

    ## PRIVATE
       In Snowflake we need to create tables outside of transactions.
       However, currently we decide to opt-out of the integrity check for
       performance reasons - there is too much overhead to each query. The check
       would fail extremely rarely so it currently does not seem worth the cost.
       We can revisit this choice in the future.
    should_check_table_integrity_at_beginning_of_transaction self -> Boolean =
        False

    ## PRIVATE
       We need custom handling for First and Last aggregations, because in
       Snowflake the `FIRST_VALUE` and `LAST_VALUE` functions in Snowflake are
       purely window functions - they cannot be directly used in aggregate
       queries.

       If tried to be used inside of a group by, it yields an error:
           Window function [FIRST_VALUE(T.A) OVER (PARTITION BY T.B ORDER BY T.C ASC NULLS LAST)] may not appear inside an aggregate function.
    custom_build_aggregate self =
        create_row_number_for orderings key_columns =
            order_descriptors = orderings.map o-> self.prepare_order_descriptor o.column o.direction text_ordering=Nothing
            key_expressions_for_orderings = key_columns.map .expression
            expression = Row_Number_Helpers.make_row_number 1 1 order_descriptors key_expressions_for_orderings
            Pair.new "row-number" expression

        create_helper_expressions aggregate_column key_columns =
            make_helper_for_first_last order_by =
                if Aggregate_Helper.is_non_empty_selector order_by then [create_row_number_for order_by key_columns] else []
            case aggregate_column of
                Aggregate_Column.First _ _ _ order_by -> make_helper_for_first_last order_by
                Aggregate_Column.Last _ _ _ order_by  -> make_helper_for_first_last order_by
                _ -> []

        make_aggregate aggregate_column as helper_expressions base_table infer_return_type problem_builder =
            make_first_last op_kind = if Aggregate_Helper.is_non_empty_selector aggregate_column.order_by . not then Aggregate_Helper.throw_ordering_required (op_kind.to_case ..Title) else
                Runtime.assert (helper_expressions.length == 1)
                row_number = helper_expressions.first
                op = case aggregate_column.ignore_nothing of
                    False -> op_kind
                    True  -> op_kind + "_NOT_NULL"
                # We just inherit the type of the source column, as the FIRST/LAST element of a column should have the same type.
                sql_type_reference = aggregate_column.column.sql_type_reference
                Internal_Column.Value as sql_type_reference (SQL_Expression.Operation op [aggregate_column.column.expression, row_number.expression])
            case aggregate_column of
                Aggregate_Column.First _ _ _ _ -> make_first_last "FIRST"
                Aggregate_Column.Last _ _ _ _ -> make_first_last "LAST"
                _ -> Aggregate_Helper.make_aggregate_column base_table aggregate_column as self infer_return_type problem_builder

        setup = Aggregate_With_Helper_Expressions.Value create_helper_expressions make_aggregate
        setup.build self

## PRIVATE
make_dialect_operations =
    arith = [round_bankers]
    cases = [["LOWER", Base_Generator.make_function "LOWER"], ["UPPER", Base_Generator.make_function "UPPER"]]
    text = [starts_with, contains, ends_with, agg_shortest, agg_longest, make_case_sensitive, ["REPLACE", replace], left, right]+concat_ops+cases+trim_ops
    counts = [agg_count_is_null, agg_count_empty, agg_count_not_empty, ["COUNT_DISTINCT", agg_count_distinct], ["COUNT_DISTINCT_INCLUDE_NULL", agg_count_distinct_include_null]]
    arith_extensions = [is_nan, is_inf, is_finite, floating_point_div, mod_op, decimal_div, decimal_mod, ["ROW_MIN", Base_Generator.make_function "LEAST"], ["ROW_MAX", Base_Generator.make_function "GREATEST"]]
    bool = [bool_or]

    stddev_pop = ["STDDEV_POP", Base_Generator.make_function "stddev_pop"]
    stddev_samp = ["STDDEV_SAMP", Base_Generator.make_function "stddev_samp"]
    stats = [agg_median, agg_mode, agg_percentile, stddev_pop, stddev_samp]
    date_ops =
        trivial = ["year", "quarter", "month", "week", "day", "hour", "minute", "second"]
            . map name-> [name, Base_Generator.make_function name]
        fractional = [extract_just_milliseconds, extract_just_microseconds, extract_just_nanoseconds]
        other = [["day_of_year", Base_Generator.make_function "DAYOFYEAR"], ["day_of_week", Base_Generator.make_function "DAYOFWEEKISO"]]
        operations = [["date_add", make_date_add], ["date_diff", make_date_diff], ["date_trunc_to_day", make_date_trunc_to_day]]
        trivial + fractional + other + operations
    other = [["IIF", make_iif], ["RUNTIME_ERROR", make_runtime_error_op], ["ROW_NUMBER_IN_GROUP", make_row_number_in_group]]
    my_mappings = arith + text + counts + stats + arith_extensions + bool + date_ops + other + first_last_aggregators
    Base_Generator.base_dialect_operations . extend_with my_mappings

## PRIVATE
agg_count_is_null = Base_Generator.lift_unary_op "COUNT_IS_NULL" arg->
    replace_with_zero_if_null <|
        SQL_Builder.code "COUNT_IF(" ++ arg.paren ++ " IS NULL)"

## PRIVATE
agg_count_empty = Base_Generator.lift_unary_op "COUNT_EMPTY" arg->
    replace_with_zero_if_null <|
        SQL_Builder.code "COUNT_IF(" ++ arg.paren ++ " IS NULL OR " ++ arg.paren ++ " = '')"

## PRIVATE
agg_count_not_empty = Base_Generator.lift_unary_op "COUNT_NOT_EMPTY" arg->
    replace_with_zero_if_null <|
        SQL_Builder.code "COUNT_IF(" ++ arg.paren ++ " IS NOT NULL AND " ++ arg.paren ++ " != '')"

## PRIVATE
   A helper needed because Snowflake's aggregators return NULL if there were no
   rows. But for aggregators like COUNT we prefer to return 0 in such cases.
replace_with_zero_if_null expr =
    SQL_Builder.code "COALESCE(" ++ expr ++ ", 0)"

## PRIVATE
agg_median = Base_Generator.lift_unary_op "MEDIAN" arg->
    median = SQL_Builder.code "MEDIAN(" ++ arg ++ ")"
    has_nan = SQL_Builder.code "BOOLOR_AGG(" ++ arg ++ " = 'NaN'::Double)"
    SQL_Builder.code "CASE WHEN " ++ has_nan ++ " THEN 'NaN'::Double ELSE " ++ median ++ " END"

## PRIVATE
agg_mode = Base_Generator.lift_unary_op "MODE" arg->
    SQL_Builder.code "MODE(" ++ arg ++ ")"

## PRIVATE
agg_percentile = Base_Generator.lift_binary_op "PERCENTILE" p-> expr->
    percentile = SQL_Builder.code "percentile_cont(" ++ p ++ ") WITHIN GROUP (ORDER BY " ++ expr ++ ")"
    has_nan = SQL_Builder.code "BOOLOR_AGG(" ++ expr ++ " = 'NaN'::Double)"
    SQL_Builder.code "CASE WHEN " ++ has_nan ++ " THEN 'NaN' ELSE " ++ percentile ++ " END"

## PRIVATE
first_last_aggregators =
    first = make_first_aggregator reverse=False ignore_null=False
    first_not_null = make_first_aggregator reverse=False ignore_null=True
    last = make_first_aggregator reverse=True ignore_null=False
    last_not_null = make_first_aggregator reverse=True ignore_null=True
    [["FIRST", first], ["FIRST_NOT_NULL", first_not_null], ["LAST", last], ["LAST_NOT_NULL", last_not_null]]

## PRIVATE
make_first_aggregator reverse ignore_null args =
    if args.length != 2 then Panic.throw (Illegal_State.Error "Expected exactly 2 arguments.") else
        result_expr = args.first
        row_number = args.second

        method_name = if reverse then "MAX_BY" else "MIN_BY"
        # If we want to ignore null values, we replace the row number in such cases with NULL, so these rows are skipped to find a non-null one (if any).
        order_expression = case ignore_null of
            False -> row_number
            True -> SQL_Builder.code "CASE WHEN " ++ result_expr ++ " IS NULL THEN NULL ELSE " ++ row_number ++ " END"
        SQL_Builder.code (method_name + "(") ++ result_expr ++ ", " ++ order_expression ++ ")"

## PRIVATE
agg_shortest = Base_Generator.lift_unary_op "SHORTEST" arg->
     SQL_Builder.code "MIN_BY(" ++ arg ++ ", LENGTH(" ++ arg ++ "))"

## PRIVATE
agg_longest = Base_Generator.lift_unary_op "LONGEST" arg->
     SQL_Builder.code "MAX_BY(" ++ arg ++ ", LENGTH(" ++ arg ++ "))"

## PRIVATE
concat_ops =
    make_raw_concat_expr expr separator =
        SQL_Builder.code "LISTAGG(" ++ expr ++ ", " ++ separator ++ ")"
    concat = Base_Generator.make_concat make_raw_concat_expr make_contains_expr
    [["CONCAT", concat (has_quote=False)], ["CONCAT_QUOTE_IF_NEEDED", concat (has_quote=True)]]

## PRIVATE
trim_ops =
    whitespace = "' ' || CHR(9) || CHR(10) || CHR(13)"
    make_fn fn_name = Base_Generator.lift_binary_op fn_name input-> chars-> case chars of
            Nothing -> SQL_Builder.code fn_name+"(" ++ input ++ ", " ++ whitespace ++ ")"
            _ ->
                case chars.is_constant of
                    True ->
                        const = chars.extract_constant
                        if const.is_nothing || const.is_empty then SQL_Builder.code fn_name+"(" ++ input ++ ", " ++ whitespace ++ ")" else
                            SQL_Builder.code fn_name+"(" ++ input ++ ", " ++ chars ++ ")"
                    False ->
                        SQL_Builder.code "CASE WHEN " ++ chars ++ " IS NULL OR " ++ chars ++ " = '' THEN " ++ fn_name ++ "(" ++ input ++ ") ELSE " ++ fn_name ++ "(" ++ input ++ ", " ++ chars ++ ") END"
    [make_fn "TRIM", make_fn "LTRIM", make_fn "RTRIM"]

## PRIVATE
agg_count_distinct args = if args.is_empty then (Error.throw (Illegal_Argument.Error "COUNT_DISTINCT requires at least one argument.")) else
    case args.length == 1 of
        True ->
            ## A single null value will be skipped.
            SQL_Builder.code "COUNT(DISTINCT " ++ args.first ++ ")"
        False ->
            ## We do not want to ignore a row where only some values are NULL - so we coalesce them.
            coalesced_args = args.map replace_null_with_marker
            # But we want to ignore all-null tuples. So we include an additional indicator column which is NULL if all cells in the given row were NULL - excluding such rows.
            are_all_nulls = SQL_Builder.join " AND " (args.map arg-> arg.paren ++ " IS NULL")
            all_null_indicator = SQL_Builder.code "CASE WHEN " ++ are_all_nulls ++ " THEN NULL ELSE 1 END"
            SQL_Builder.code "COUNT(DISTINCT " ++ SQL_Builder.join ", " (coalesced_args + [all_null_indicator]) ++ ")"

## PRIVATE
agg_count_distinct_include_null args =
    # As with `agg_count_distinct`, we do want to handle columns that contain NULLs, so we need to apply the ugly coalesce.
    coalesced_args = args.map replace_null_with_marker
    SQL_Builder.code "COUNT(DISTINCT " ++ SQL_Builder.join ", " coalesced_args ++ ")"

## PRIVATE
   A helper function that coalesces a NULL column replacing it with a marker value that is expected to not be present in real world data.
   It is sometimes needed when we want to count distinct values in a column that contains NULLs and still include the rows containing NULLs.
   The columns are converted to VARIANT type because of that, which may incur some overhead.
   But there seems to be no other reliable way to handle this for columns like numeric where no non-NULL value exists that can be guaranteed to be unused.
replace_null_with_marker expr =
    SQL_Builder.code "COALESCE(" ++ expr ++ "::variant, {'enso-null-replacement-marker':'" ++ Random.uuid ++ "'}::variant)"

## PRIVATE
   A helper for `lookup_and_replace`, and perhaps other operation.
   It creates an expression that returns a row number within a group.
   This is a specialization for Snowflake that adds a dummy ORDER BY clause to satisfy its compiler.
make_row_number_in_group arguments =
    if arguments.length == 0 then
        Panic.throw <| Illegal_State.Error "The operation ROW_NUMBER_IN_GROUP requires at least one argument."
    SQL_Builder.code "ROW_NUMBER() OVER (PARTITION BY " ++ (SQL_Builder.join ", " arguments) ++ " ORDER BY 1)"


## PRIVATE
starts_with = Base_Generator.lift_binary_sql_function "STARTS_WITH" "STARTSWITH"

## PRIVATE
ends_with = Base_Generator.lift_binary_sql_function "ENDS_WITH" "ENDSWITH"

## PRIVATE
contains = Base_Generator.lift_binary_sql_function "CONTAINS" "CONTAINS"

## PRIVATE
make_contains_expr expr substring = contains.second [expr, substring]

## PRIVATE
make_case_sensitive = Base_Generator.lift_unary_op "MAKE_CASE_SENSITIVE" arg->
    SQL_Builder.code "((" ++ arg ++ ") COLLATE 'ucs_basic')"

## PRIVATE
left = Base_Generator.lift_binary_op "LEFT" str-> n->
    SQL_Builder.code "left(" ++ str ++ ", CAST(" ++ n ++ " AS INT))"

## PRIVATE
right = Base_Generator.lift_binary_op "RIGHT" str-> n->
    SQL_Builder.code "right(" ++ str ++ ", CAST(" ++ n ++ " AS INT))"

## PRIVATE
make_order_descriptor internal_column sort_direction text_ordering =
    nulls = case sort_direction of
        Sort_Direction.Ascending -> Nulls_Order.First
        Sort_Direction.Descending -> Nulls_Order.Last
    case text_ordering of
        Nothing ->
            Order_Descriptor.Value (Internals_Access.column_expression internal_column) sort_direction nulls_order=nulls collation=Nothing
        _ ->
            ## In the future we can modify this error to suggest using a custom defined collation.
            if text_ordering.sort_digits_as_numbers then Error.throw (Unsupported_Database_Operation.Error "Natural ordering") else
                case text_ordering.case_sensitivity of
                    Case_Sensitivity.Default ->
                        Order_Descriptor.Value (Internals_Access.column_expression internal_column) sort_direction nulls_order=nulls collation=Nothing
                    Case_Sensitivity.Sensitive ->
                        # Order_Descriptor.Value (Internals_Access.column_expression internal_column) sort_direction nulls_order=nulls collation="ucs_basic"
                        Error.throw ("Feature unsupported due to bug. Please use `..Default` until https://github.com/enso-org/enso/issues/10835 is fixed.")
                    Case_Sensitivity.Insensitive locale -> case locale == Locale.default of
                        False ->
                            Error.throw (Unsupported_Database_Operation.Error "Case insensitive ordering with custom locale")
                        True ->
                            upper = SQL_Expression.Operation "UPPER" [Internals_Access.column_expression internal_column]
                            folded_expression = SQL_Expression.Operation "LOWER" [upper]
                            Order_Descriptor.Value folded_expression sort_direction nulls_order=nulls collation=Nothing

## PRIVATE
round_bankers = Base_Generator.lift_binary_op "ROUND_BANKERS" x-> dp->
    two = SQL_Builder.code "2"
    ten = SQL_Builder.code "10"
    fun f args = (SQL_Builder.code f ++ "(" ++ (SQL_Builder.join "," (args.map .paren)) ++ ")").paren
    neg x = (SQL_Builder.code "-" ++ x.paren).paren
    scale = fun "POW" [ten, dp]
    floor x = fun "FLOOR" [x]
    ceil x = fun "CEIL" [x]
    round x = fun "ROUND" [x]
    case_when cases else_clause =
        cases_sqls = cases.map pr->
            pred = pr.at 0
            exp = pr.at 1
            SQL_Builder.code "WHEN " ++ pred.paren ++ " THEN " ++ exp.paren
        (SQL_Builder.code "CASE " ++ (SQL_Builder.join " " cases_sqls) ++ " ELSE " ++ else_clause ++ " END").paren
    mod2 x = fun "MOD" [x, two]

    # Scale for the decimal_places argument.
    scaled = (x.paren ++ " * " ++ scale).paren

    # We've wrapped all values to the 0..2 range, which needs 3 cases.
    partition x = case_when [[mod2 x ++ " <= 0.5", floor x], [mod2 x ++ " >= 1.5", ceil x]] (round x)

    # Handle negative values by negating on the way in and on the way out.
    flip_for_negative x f = case_when [[x ++ " >= 0", f x]] (neg (f (neg x)))
    bankers_rounded = flip_for_negative scaled partition

    # Undo the scaling factor.
    (bankers_rounded ++ " / " ++ scale).paren

## PRIVATE
is_nan = Base_Generator.lift_unary_op "IS_NAN" arg->
    (arg ++ " in ('NaN'::float)").paren

## PRIVATE
is_inf = Base_Generator.lift_unary_op "IS_INF" arg->
    (arg ++ " in ('Infinity'::float, '-Infinity'::float)").paren

## PRIVATE
is_finite = Base_Generator.lift_unary_op "IS_FINITE" arg->
    (arg ++ " not in ('NaN'::float, 'Infinity'::float, '-Infinity'::float)").paren

## PRIVATE
bool_or = Base_Generator.lift_unary_op "BOOL_OR" arg->
    SQL_Builder.code "bool_or(" ++ arg ++ ")"

## PRIVATE
floating_point_div = Base_Generator.lift_binary_op "/" x-> y->
    SQL_Builder.code "CAST(" ++ x ++ " AS float) / CAST(" ++ y ++ " AS float)"

## PRIVATE
mod_op = Base_Generator.lift_binary_op "MOD" x-> y->
    x ++ " % " ++ y

## PRIVATE
decimal_div = Base_Generator.lift_binary_op "DECIMAL_DIV" x-> y->
    x ++ " / " ++ y

## PRIVATE
decimal_mod = Base_Generator.lift_binary_op "DECIMAL_MOD" x-> y->
    x ++ " % " ++ y

## PRIVATE
supported_replace_params : Hashset Replace_Params
supported_replace_params =
    e0 = [Replace_Params.Value Text Case_Sensitivity.Default False, Replace_Params.Value Text Case_Sensitivity.Default True, Replace_Params.Value Text Case_Sensitivity.Sensitive False]
    e1 = [Replace_Params.Value Text Case_Sensitivity.Sensitive True, Replace_Params.Value Text Case_Sensitivity.Insensitive False, Replace_Params.Value Text Case_Sensitivity.Insensitive True]
    e2 = [Replace_Params.Value Regex Case_Sensitivity.Default False, Replace_Params.Value Regex Case_Sensitivity.Default True, Replace_Params.Value Regex Case_Sensitivity.Sensitive False]
    e3 = [Replace_Params.Value Regex Case_Sensitivity.Sensitive True, Replace_Params.Value Regex Case_Sensitivity.Insensitive False, Replace_Params.Value Regex Case_Sensitivity.Insensitive True]
    e4 = [Replace_Params.Value DB_Column Case_Sensitivity.Default False, Replace_Params.Value DB_Column Case_Sensitivity.Sensitive False]
    Hashset.from_vector <| e0 + e1 + e2 + e3 + e4

## PRIVATE
replace : Vector SQL_Builder -> Any -> SQL_Builder
replace args metadata =
    input = args.at 0
    pattern = args.at 1
    replacement = args.at 2

    ## `raw_pattern` is a `Text1 or `Regex`; it's the same value as `input`, but not
       embedded in IR.
    raw_pattern = metadata.at 0
    replace_params = metadata.at 1

    ## The REGEXP_REPLACE function in Snowflake takes the following parameters:
       <subject>, <pattern> [, <replacement>, <position>, <occurrence>, <parameters>

       The `position` starts at 1 to search the whole string. Defaults to 1.
       If `occurrence` is set to 0 all occurrences are replaced, otherwise the first N are replaced. Defaults to 0.
       See: https://docs.snowflake.com/en/sql-reference/functions/regexp_replace
    expression = case replace_params.input_type of
        Text ->
            ## To use REGEXP_REPLACE on a non-regex, we have to escape it.
            escaped_pattern = SQL_Builder.interpolation (Regex.escape raw_pattern)
            case replace_params.only_first of
                False -> case replace_params.case_sensitivity of
                    Case_Sensitivity.Insensitive _ ->
                        SQL_Builder.code "REGEXP_REPLACE(" ++ input ++ ", " ++ escaped_pattern ++ ", " ++ replacement ++ ", 1, 0, 'i')"
                    _ ->
                        SQL_Builder.code "REPLACE(" ++ input ++ ", " ++ pattern ++ ", " ++ replacement ++ ")"
                True -> case replace_params.case_sensitivity of
                    Case_Sensitivity.Insensitive _ ->
                        SQL_Builder.code "REGEXP_REPLACE(" ++ input ++ ", " ++ escaped_pattern ++ ", " ++ replacement ++ ", 1, 1, 'i')"
                    _ ->
                        SQL_Builder.code "REGEXP_REPLACE(" ++ input ++ ", " ++ escaped_pattern ++ ", " ++ replacement ++ ", 1, 1)"
        Regex ->
            pattern_text = SQL_Builder.interpolation raw_pattern.pattern
            case replace_params.only_first of
                False -> case replace_params.case_sensitivity of
                    Case_Sensitivity.Insensitive _ ->
                        SQL_Builder.code "REGEXP_REPLACE(" ++ input ++ ", " ++ pattern_text ++ ", " ++ replacement ++ ", 1, 0, 'i')"
                    _ ->
                        SQL_Builder.code "REGEXP_REPLACE(" ++ input ++ ", " ++ pattern_text ++ ", " ++ replacement ++ ")"
                True -> case replace_params.case_sensitivity of
                    Case_Sensitivity.Insensitive _ ->
                        SQL_Builder.code "REGEXP_REPLACE(" ++ input ++ ", " ++ pattern_text ++ ", " ++ replacement ++ ", 1, 1, 'i')"
                    _ ->
                        SQL_Builder.code "REGEXP_REPLACE(" ++ input ++ ", " ++ pattern_text ++ ", " ++ replacement ++ ", 1, 1)"
        DB_Column ->
            case replace_params.only_first of
                False -> case replace_params.case_sensitivity of
                    Case_Sensitivity.Insensitive _ ->
                        Nothing
                    _ ->
                        SQL_Builder.code "REPLACE(" ++ input ++ ", " ++ pattern ++ ", " ++ replacement ++ ")"
                True -> Nothing
    expression.if_nothing (replace_params.throw_unsupported snowflake_dialect_name)

extract_just_nanoseconds = Base_Generator.lift_unary_op "nanosecond" arg->
    SQL_Builder.code "(EXTRACT(NANOSECOND FROM " ++ arg ++ ") % 1000)"

extract_just_microseconds = Base_Generator.lift_unary_op "microsecond" arg->
    SQL_Builder.code "(TRUNC(EXTRACT(NANOSECOND FROM " ++ arg ++ ") / 1000) % 1000)"

extract_just_milliseconds = Base_Generator.lift_unary_op "millisecond" arg->
    # No modulo is needed, as the milliseconds are the last part of the nanoseconds.
    SQL_Builder.code "TRUNC(EXTRACT(NANOSECOND FROM " ++ arg ++ ") / 1000000)"

## PRIVATE
date_period_to_part_with_multiplier period =
    case period of
        Date_Period.Year    -> ["year", 1]
        Date_Period.Quarter -> ["quarter", 1]
        Date_Period.Month   -> ["month", 1]
        Date_Period.Week _  -> ["week", 1]
        Date_Period.Day     -> ["day", 1]
        Time_Period.Hour    -> ["hour", 1]
        Time_Period.Minute  -> ["minute", 1]
        Time_Period.Second  -> ["second", 1]
        Time_Period.Millisecond -> ["millisecond", 1]
        Time_Period.Microsecond -> ["microsecond", 1]
        Time_Period.Nanosecond  -> ["nanosecond", 1]

## PRIVATE
make_date_add arguments (metadata : Date_Period_Metadata) =
    if arguments.length != 2 then Error.throw (Illegal_State.Error "date_add expects exactly 2 sub expressions. This is a bug in Database library.") else
        expr = arguments.at 0
        amount = arguments.at 1
        part_with_multiplier = date_period_to_part_with_multiplier metadata.period
        date_part = part_with_multiplier.first
        multiplier = part_with_multiplier.second
        scaled_amount = if multiplier == 1 then amount else
            amount ++ " * " ++ multiplier.to_text
        sql_typ = sql_type_string_for_date_time metadata.input_value_type
        SQL_Builder.code "DATEADD('"+date_part+"', (" ++ scaled_amount ++ ")::NUMBER, (" ++ expr ++ ")::" ++ sql_typ ++ ")"

## PRIVATE
make_date_diff arguments (metadata : Date_Period_Metadata) =
    if arguments.length != 2 then Error.throw (Illegal_State.Error "date_diff expects exactly 2 sub expressions. This is a bug in Database library.") else
        start = arguments.at 0
        end = arguments.at 1
        case metadata.period of
            # Year / Quarter / Month rely on MONTHS_BETWEEN because DATEDIFF looked just at months and years and ignored days, yielding incorrect results.
            Date_Period.Year ->
                SQL_Builder.code "TRUNC(MONTHS_BETWEEN((" ++ end ++ ")::DATE, (" ++ start ++ ")::DATE) / 12)"
            Date_Period.Quarter ->
                SQL_Builder.code "TRUNC(MONTHS_BETWEEN((" ++ end ++ ")::DATE, (" ++ start ++ ")::DATE) / 3)"
            Date_Period.Month ->
                SQL_Builder.code "TRUNC(MONTHS_BETWEEN((" ++ end ++ ")::DATE, (" ++ start ++ ")::DATE))"
            _ ->
                part_with_multiplier = date_period_to_part_with_multiplier metadata.period
                date_part = part_with_multiplier.first
                multiplier = part_with_multiplier.second
                ## The SQL type to add as a cast. This is needed, because otherwise this operation is losing type information,
                   especially if given NULL (Nothing). It would tell that it returns a VARCHAR which is not true.
                sql_typ = sql_type_string_for_date_time metadata.input_value_type
                diff = SQL_Builder.code "DATEDIFF('" ++ date_part ++ "', (" ++ start ++ ")::" ++ sql_typ ++ ", (" ++ end ++ ")::" ++ sql_typ ++ ")"
                if multiplier == 1 then diff else
                    # We want to return integer, so we truncate any fractional part that did not constitute a full unit.
                    SQL_Builder.code "TRUNC(" ++ diff ++ " / " ++ multiplier.to_text ++ ")"

## PRIVATE
make_date_trunc_to_day arguments =
    if arguments.length != 1 then Error.throw (Illegal_State.Error "date_trunc_to_day expects exactly one sub expression. This is a bug in Database library.") else
        expr = arguments.at 0
        SQL_Builder.code "(DATE_TRUNC('day'," ++ expr ++ ") :: DATE)"

## PRIVATE
sql_type_string_for_date_time value_type = case value_type of
    Value_Type.Date -> "DATE"
    Value_Type.Date_Time with_tz -> if with_tz then "TIMESTAMP_TZ" else "TIMESTAMP_NTZ"
    Value_Type.Time -> "TIME"
    _ -> Panic.throw (Illegal_State.Error "Expects a date or time type. This is a bug in Database library.")

## PRIVATE
   The RUNTIME_ERROR operation should allow the query to compile fine and it
   will not prevent it from running if the branch including this operation is
   not taken. But if the branch is computed, it should ensure the query fails.

   This query never returns a value, so its type should be polymorphic. However,
   that is not possible - so currently it just 'pretends' that it would return a
   Boolean - because that is the type we expect in the use-case. This can be
   altered if needed.

   It takes a variable as the second argument. It can be any value that is not
   statically known - this ensure that the optimizer will not be able to
   pre-compute the expression too early (which could make the query fail
   spuriously). See `make_invariant_check` in `Lookup_Query_Helper` for an
   example.
make_runtime_error_op arguments =
    if arguments.length != 2 then
        Panic.throw (Illegal_Argument.Error "RUNTIME_ERROR takes exactly 2 arguments (error message and a variable to ensure deferred execution).")
    error_message = arguments.at 0
    variable_to_defer = arguments.at 1

    SQL_Builder.code "CAST('[ENSO INVARIANT VIOLATED: '||" ++ error_message ++ "||'] '||COALESCE(" ++ variable_to_defer ++ "::TEXT,'NULL') AS BOOLEAN)"

## PRIVATE
make_iif : Vector SQL_Builder -> SQL_Builder
make_iif arguments = case arguments.length of
    3 ->
        expr = arguments.at 0
        when_true = arguments.at 1
        when_false = arguments.at 2
        # We can rely on Snowflake's decode to avoid duplicating `expr` in the SQL code:
        # (if no default fallback is provided, NULL will be returned - meaning that NULL is mapped to NULL as expected)
        SQL_Builder.code "DECODE(" ++ expr ++ ", TRUE, " ++ when_true ++ ", FALSE, " ++ when_false ++ ")"
    _ ->
        Error.throw <| Illegal_State.Error ("Invalid amount of arguments for operation IIF")

## PRIVATE
make_distinct_extension distinct_expressions =
    run_generator sql_expressions =
        joined = SQL_Builder.join ", " sql_expressions
        ## We could use the ORDER BY here to ensure any previous ordering is preserved by distinct.
           But to do so we need to have a robust way of checking such ordering, even if subqueries were taken in the meantime.
           This may be related to #10321
           Once fixed, should re-enable the `distinct_returns_first_row_from_group_if_ordered` flag back to `True`.
        SQL_Builder.code " QUALIFY ROW_NUMBER() OVER (PARTITION BY " ++ joined ++ " ORDER BY 1) = 1 "
    Context_Extension.Value position=550 expressions=distinct_expressions run_generator=run_generator

## PRIVATE
   Returns a custom cast expression if it is needed for a specific pair of types,
   or Nothing if the default cast is sufficient.
make_custom_cast (column : Internal_Column) (source_value_type : Value_Type) (target_value_type : Value_Type) -> SQL_Expression | Nothing =
    result = Ref.new Nothing

    # Custom expression for boolean to float cast, as regular cast does not support it.
    if source_value_type.is_boolean && target_value_type.is_floating_point then
        result.put <|
            SQL_Expression.Operation "IIF" [Internals_Access.column_expression column, SQL_Expression.Literal "1", SQL_Expression.Literal "0"]

    # If the text length is bounded, we need to add a `LEFT` call to truncate to desired length avoiding errors.
    if target_value_type.is_text && target_value_type.size.is_nothing.not then
        # But we only do so if the source type was also text.
        # For any other source type, we keep the original behaviour - failing to convert if the text representation would not fit.
        if source_value_type.is_text then result.put <|
            max_size = (target_value_type.size : Integer)
            truncated = SQL_Expression.Operation "LEFT" [Internals_Access.column_expression column, SQL_Expression.Literal max_size.to_text]
            # We still need a cast to ensure the Value_Type gets the max size in it - LEFT returns no size limit unfortunately.
            target_type_name = "VARCHAR(" + max_size.to_text + ")"
            SQL_Expression.Operation "CAST" [truncated, SQL_Expression.Literal target_type_name]

    result.get

## PRIVATE
snowflake_dialect_name = "Snowflake"
