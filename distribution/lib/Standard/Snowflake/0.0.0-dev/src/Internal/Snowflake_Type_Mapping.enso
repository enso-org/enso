private

from Standard.Base import all
import Standard.Base.Data.Numbers.Positive_Integer
import Standard.Base.Errors.Illegal_Argument.Illegal_Argument

import Standard.Table.Internal.Java_Exports
from Standard.Table import Bits, Value_Type
from Standard.Table.Errors import Inexact_Type_Coercion

import Standard.Database.Internal.Column_Fetcher as Column_Fetcher_Module
import Standard.Database.Internal.Column_Fetcher.Column_Fetcher
import Standard.Database.Internal.IR.SQL_Expression.SQL_Expression
import Standard.Database.Internal.SQL_Type_Mapping
import Standard.Database.Internal.SQL_Type_Reference.SQL_Type_Reference
import Standard.Database.SQL_Type.SQL_Type
from Standard.Database.Errors import Unsupported_Database_Operation

polyglot java import java.sql.Types
polyglot java import org.enso.snowflake.SnowflakeIntegerColumnMaterializer

## PRIVATE
type Snowflake_Type_Mapping
    ## PRIVATE
    value_type_to_sql : Value_Type -> Problem_Behavior -> SQL_Type
    value_type_to_sql value_type on_problems:Problem_Behavior =
        result = case value_type of
            Value_Type.Boolean -> SQL_Type.Value Types.BOOLEAN "boolean"
            # All integer types in Snowflake become NUMERIC(38,0).
            Value_Type.Byte -> integer_type
            Value_Type.Integer _ -> integer_type
            # All float types in Snowflake become double
            Value_Type.Float _ -> SQL_Type.Value Types.DOUBLE "FLOAT"
            Value_Type.Decimal precision scale -> case precision of
                # If precision is not set, scale is also lost because SQL is unable to express a scale without a precision.
                Nothing -> SQL_Type.Value Types.DECIMAL "NUMBER" 38 12
                # Scale can be set or not, but if it is set, it must be in range 0-37.
                # If scale or precision is out of range, we fall back to Nothing.
                _       -> if (precision < 1) || (precision > 38) then SQL_Type.Value Types.DECIMAL "NUMBER" Nothing Nothing else
                    if scale.is_nothing then SQL_Type.Value Types.DECIMAL "NUMBER" precision Nothing else
                        if (scale < 0) || (scale > 37) then SQL_Type.Value Types.DECIMAL "NUMBER" Nothing Nothing else
                            SQL_Type.Value Types.DECIMAL "NUMBER" precision scale

            Value_Type.Char size _ ->
                # Snowflake does not support fixed length strings, so we use VARCHAR.
                is_unbounded = case size of
                    Nothing -> True
                    Positive_Integer.Value integer -> integer >= max_length
                case is_unbounded of
                    True  -> SQL_Type.Value Types.VARCHAR "varchar"
                    False -> SQL_Type.Value Types.VARCHAR "varchar" size
            Value_Type.Time -> SQL_Type.Value Types.TIME "time"
            Value_Type.Date -> SQL_Type.Value Types.DATE "date"
            Value_Type.Date_Time with_timezone ->
                type_name = if with_timezone then "timestamp_tz" else "timestamp_ntz"
                SQL_Type.Value Types.TIMESTAMP type_name
            Value_Type.Binary size _ ->
                ## Snowflake does not support fixed length binary types, so we use BINARY.
                is_unbounded = case size of
                    Nothing -> True
                    Positive_Integer.Value integer -> integer >= max_length
                case is_unbounded of
                    True  -> SQL_Type.Value Types.BINARY "binary"
                    False -> SQL_Type.Value Types.BINARY "binary" size
            Value_Type.Mixed -> Error.throw (Unsupported_Database_Operation.Error "Snowflake tables do not support Mixed types.")
            Value_Type.Unsupported_Data_Type type_name underlying_type ->
                underlying_type.if_nothing <| Error.throw <| Illegal_Argument.Error <|
                    "An unsupported SQL type ["+type_name.to_text+"] cannot be converted into an SQL type because it did not contain the SQL metadata needed to reconstruct it."

        approximated_value_type = Snowflake_Type_Mapping.sql_type_to_value_type result
        problems = if approximated_value_type == value_type then [] else
            # We skip the inexact coercion warning if the conversion is an implicit one.
            if Snowflake_Type_Mapping.is_implicit_conversion value_type approximated_value_type then [] else
                [Inexact_Type_Coercion.Warning value_type approximated_value_type]
        on_problems.attach_problems_before problems result

    ## PRIVATE
    sql_type_to_value_type : SQL_Type -> Value_Type
    sql_type_to_value_type sql_type =
        simple_type = simple_types_map.get sql_type.typeid Nothing
        simple_type.if_nothing <|
            ## If we didn't match any of the types from the simple mapping, we
               continue with the more complex mappings that take stuff like
               precision into account.
            case complex_types_map.get sql_type.typeid Nothing of
                Nothing -> on_unknown_type sql_type
                builder -> builder sql_type

    ## PRIVATE
    sql_type_to_text : SQL_Type -> Text
    sql_type_to_text sql_type =
        variable_length_types = [Types.VARCHAR, Types.BINARY]
        ## If the type is variable length and the maximum is provided, we treat
           it as unbounded, otherwise too big max length may be not accepted by
           Snowflake.
        skip_precision = (variable_length_types.contains sql_type.typeid) && (sql_type.precision == max_length)
        case skip_precision of
            True -> sql_type.name
            False -> SQL_Type_Mapping.default_sql_type_to_text sql_type

    ## PRIVATE
       The Snowflake_Type_Mapping always relies on the return type determined by
       the database backend.
    infer_return_type : (SQL_Expression -> SQL_Type_Reference) -> Text -> Vector -> SQL_Expression -> SQL_Type_Reference
    infer_return_type infer_from_database_callback op_name arguments expression =
        _ = [op_name, arguments]
        infer_from_database_callback expression

    ## PRIVATE
       We want to respect any overriding references, but references that rely on
       computing the type by the database are resolved to Nothing to just rely
       on the `ResultSet` metadata and decrease overhead.
    prepare_type_overrides : Nothing | Vector SQL_Type_Reference -> Nothing | Vector (Nothing | SQL_Type)
    prepare_type_overrides column_type_suggestions = case column_type_suggestions of
        Nothing -> Nothing
        _ : Vector -> column_type_suggestions.map .to_type_override

    ## PRIVATE
       Creates a `Column_Fetcher` used to fetch data from a result set and build
       an in-memory column from it, based on the given column type.
    make_column_fetcher : SQL_Type -> Column_Fetcher
    make_column_fetcher self sql_type =
        value_type = self.sql_type_to_value_type sql_type
        case value_type of
            Value_Type.Time -> time_fetcher
            Value_Type.Date_Time _ -> date_time_fetcher
            # If we encounter a Decimal column with scale 0 we will try to fetch it as Integer if the values fit.
            Value_Type.Decimal _ 0 -> smart_integer_fetcher
            # Other Decimal columns get the default behaviour - fetched as BigInteger or BigDecimal.
            _ -> Column_Fetcher_Module.default_fetcher_for_value_type value_type

    ## PRIVATE
    is_implicit_conversion (source_type : Value_Type) (target_type : Value_Type) -> Boolean =
        # Currently only implicit conversion is Integer -> Decimal
        case source_type of
            Value_Type.Integer _ ->
                 target_type == integer_value_type
            _ -> False

    ## PRIVATE
    should_warn_on_materialize (db_type : Value_Type) (in_memory_type : Value_Type) -> Boolean =
        is_decimal_to_integer = db_type.is_decimal && db_type.scale == 0 && in_memory_type.is_integer
        # We don't warn about the expected conversion of small integer column to Integer in-memory.
        if is_decimal_to_integer then False else
            # For other types, we delegate to the default behavior.
            SQL_Type_Mapping.default_should_warn_on_materialize db_type in_memory_type

    ## PRIVATE
    is_integer_type (value_type : Value_Type) -> Boolean = case value_type of
        Value_Type.Integer _ -> True
        Value_Type.Decimal _ scale -> scale == 0
        _ -> False

    ## PRIVATE
    is_same_type (value_type1 : Value_Type) (value_type2 : Value_Type) -> Boolean =
        # Types are considered the same by original semantics, or additionally if both of them are an integer-like type as denoted by `is_integer_type`.
        (value_type1.is_same_type value_type2) || (Snowflake_Type_Mapping.is_integer_type value_type1 && Snowflake_Type_Mapping.is_integer_type value_type2)

## PRIVATE
simple_types_map = Dictionary.from_vector <|
    floats = [[Types.DOUBLE, Value_Type.Float Bits.Bits_64], [Types.REAL, Value_Type.Float Bits.Bits_64]]
    other = [[Types.DATE, Value_Type.Date], [Types.TIME, Value_Type.Time], [Types.BOOLEAN, Value_Type.Boolean]]
    floats + other

## PRIVATE
complex_types_map = Dictionary.from_vector <|
    make_decimal sql_type =
        Value_Type.Decimal sql_type.precision sql_type.scale
    make_varchar sql_type =
        effective_size = if sql_type.precision==max_length || (sql_type.precision==9 && sql_type.scale==9) then Nothing else sql_type.precision
        Value_Type.Char size=effective_size variable_length=True
    make_char sql_type =
        # Even the CHAR type in Snowflake is still variable length - it is just an alias for VARCHAR.
        size = sql_type.precision.if_nothing 1
        Value_Type.Char size=size variable_length=True
    make_binary variable sql_type =
        Value_Type.Binary size=sql_type.precision variable_length=variable
    handle_bit sql_type =
        if sql_type.name == "BOOLEAN" then Value_Type.Boolean else
            # We currently do not support bit types.
            on_unknown_type sql_type
    handle_timestamp sql_type = case sql_type.name.to_case Case.Upper of
        "TIMESTAMPTZ" -> Value_Type.Date_Time with_timezone=True
        "TIMESTAMP_TZ" -> Value_Type.Date_Time with_timezone=True
        "TIMESTAMPLTZ" -> Value_Type.Date_Time with_timezone=False
        "TIMESTAMP_LTZ" -> Value_Type.Date_Time with_timezone=False
        "TIMESTAMPNTZ"   -> Value_Type.Date_Time with_timezone=False
        "TIMESTAMP_NTZ"   -> Value_Type.Date_Time with_timezone=False
        _             -> on_unknown_type sql_type

    # All integer types in Snowflake are Decimal
    numerics = [[Types.DECIMAL, make_decimal], [Types.NUMERIC, make_decimal], [Types.BIGINT, make_decimal]]
    strings = [[Types.VARCHAR, make_varchar], [Types.CHAR, make_char], [Types.CLOB, make_varchar]]
    binaries = [[Types.BINARY, make_binary True], [Types.BIT, handle_bit]]
    others = [[Types.TIMESTAMP, handle_timestamp], [Types.TIMESTAMP_WITH_TIMEZONE, handle_timestamp]]
    numerics + strings + binaries + others

## PRIVATE
on_unknown_type sql_type =
    Value_Type.Unsupported_Data_Type sql_type.name sql_type

## PRIVATE
   This is the maximum size that JDBC driver reports for 'unbounded' types in
   Snowflake.
max_length = 16777216

## PRIVATE
time_fetcher =
    fetch_value rs i =
        ## Read the time as a string to get the nanosecond precision.
        sf_string = rs.getString i
        if sf_string == Nothing then Nothing else Time_Of_Day.parse sf_string
    make_builder initial_size _ =
        java_builder = Java_Exports.make_time_of_day_builder initial_size
        Column_Fetcher_Module.make_builder_from_java_object_builder java_builder
    Column_Fetcher.Value fetch_value make_builder

## PRIVATE
date_time_fetcher =
    fetch_value rs i =
        ## Read the time as a string to get the nanosecond precision.
        sf_string = rs.getString i
        if sf_string == Nothing then Nothing else
            normalized = if sf_string.at 10 != 'T' then sf_string else
                (sf_string.take 10) + ' ' + (sf_string.drop 11)
            # The offset is optional - if we were fetching TIMESTAMP_NTZ it could be missing.
            # The two variants of offset are needed - to handle both `+0200` and `+02:00` formats.
            Date_Time.parse normalized "yyyy-MM-dd HH:mm:ss.f[ ZZ][ ZZZZZ]"
    make_builder initial_size _ =
        java_builder = Java_Exports.make_date_time_builder initial_size
        Column_Fetcher_Module.make_builder_from_java_object_builder java_builder
    Column_Fetcher.Value fetch_value make_builder

## PRIVATE
   A fetcher for Snowflake Decimal integer columns.
   Integer columns in Snowflake are represented as `NUMBER(38, 0)`, meaning
   there is no separate Integer type.

   In Enso, using `Decimal` values incurs a significant overhead. Thus, when
   fetching such an integer column from Snowflake, we try to first fetch it as
   lightweight `Integer` and only fall back to `Decimal` if needed.
smart_integer_fetcher =
    make_builder initial_size _ =
        java_builder = SnowflakeIntegerColumnMaterializer.new initial_size
        Column_Fetcher_Module.make_builder_from_java_object_builder java_builder
    Column_Fetcher.Value Column_Fetcher_Module.fetch_big_integer make_builder

## PRIVATE
   The actual SQL type that Snowflake uses for all integer types.
integer_type = SQL_Type.Value Types.DECIMAL "NUMERIC" 38 0

## PRIVATE
integer_value_type = Value_Type.Decimal 38 0

## PRIVATE
float_value_type = Value_Type.Float Bits.Bits_64
