from Standard.Base import all
import Standard.Base.Errors.Common.Forbidden_Operation
import Standard.Base.Errors.Common.Syntax_Error
import Standard.Base.Errors.File_Error.File_Error
import Standard.Base.Errors.Illegal_Argument.Illegal_Argument
import Standard.Base.Errors.Unimplemented.Unimplemented
import Standard.Base.Runtime.Context
import Standard.Base.System.File.Generic.File_Like.File_Like
import Standard.Base.System.File_Format_Metadata.File_Format_Metadata
import Standard.Base.System.File.Generic.Writable_File.Writable_File
import Standard.Base.System.Input_Stream.Input_Stream
import Standard.Base.System.Output_Stream.Output_Stream
from Standard.Base.System.File import find_extension_from_name
from Standard.Base.System.File.Generic.File_Write_Strategy import generic_copy

import project.AWS_Credential.AWS_Credential
import project.Errors.S3_Error
import project.Errors.S3_Key_Not_Found
import project.Internal.S3_File_Write_Strategy
import project.S3.S3

## Represents an S3 file or folder
   If the path ends with a slash, it is a folder. Otherwise, it is a file.
type S3_File
    ## Given an S3 URI create a file representation.

       Arguments:
       - uri: The URI of the file.
         The URI must be in the form `s3://bucket/path/to/file`.
       - credentials: The credentials to use when accessing the file.
         If not specified, the default credentials are used.
    new : Text -> AWS_Credential | Nothing -> S3_File
    new uri=S3.uri_prefix credentials=Nothing =
        parts = S3.parse_uri uri
        if parts.is_nothing then Error.throw (Syntax_Error.Error "Invalid S3 URI.") else
            S3_File.Value parts.first parts.second credentials

    ## PRIVATE
    Value bucket:Text prefix:Text credentials:(AWS_Credential | Nothing)

    ## GROUP Standard.Base.Metadata
       Gets the URI of this file
    uri : Text
    uri self = S3.uri_prefix + self.bucket + "/" + self.prefix

    ## GROUP Standard.Base.Metadata
       Checks if the folder or file exists
    exists : Boolean
    exists self = if self.bucket == "" then True else
        if self.prefix == "" then translate_file_errors self <| S3.head self.bucket "" self.credentials . is_error . not else
            pair = translate_file_errors self <| S3.read_bucket self.bucket self.prefix self.credentials max_count=1
            pair.second.length > 0

    ## GROUP Standard.Base.Metadata
       Checks if this is a folder.
    is_directory : Boolean
    is_directory self = (self.prefix == "") || (self.prefix.ends_with "/")

    ## GROUP Standard.Base.Metadata
       Checks if this is a regular file.
    is_regular_file : Boolean
    is_regular_file self = self.is_directory.not

    ## GROUP Standard.Base.Metadata
       Gets the size of a file in bytes.
    size : Integer
    size self =
        if self.is_directory then Error.throw (S3_Error.Error "size can only be called on files." self.uri) else
            content_length = translate_file_errors self <| S3.raw_head self.bucket self.prefix self.credentials . contentLength
            if content_length.is_nothing then Error.throw (S3_Error.Error "ContentLength header is missing." self.uri) else content_length

    ## PRIVATE
       ADVANCED
       Creates a new output stream for this file and runs the specified action
       on it.

       The created stream is automatically closed when `action` returns (even
       if it returns exceptionally).

       Arguments:
       - open_options: A vector of `File_Access` objects determining how to open
         the stream. These options set the access properties of the stream.
       - action: A function that operates on the output stream and returns some
         value. The value is returned from this method.
    with_output_stream : Vector File_Access -> (Output_Stream -> Any ! File_Error) -> Any ! File_Error
    with_output_stream self (open_options : Vector) action = if self.is_directory then Error.throw (S3_Error.Error "S3 directory cannot be opened as a stream." self.uri) else
        if Context.Output.is_enabled.not then Error.throw (Forbidden_Operation.Error "Writing to an S3_File is forbidden as the Output context is disabled.") else
            if open_options.contains File_Access.Append then Error.throw (S3_Error.Error "S3 does not support appending to a file. Instead you may read it, modify and then write the new contents." self.uri) else
                # The exists check is not atomic, but it is the best we can do with S3
                check_exists = open_options.contains File_Access.Create_New
                valid_options = [File_Access.Write, File_Access.Create_New, File_Access.Truncate_Existing, File_Access.Create]
                invalid_options = open_options.filter (Filter_Condition.Is_In valid_options action=Filter_Action.Remove)
                if invalid_options.not_empty then Error.throw (S3_Error.Error "Unsupported S3 stream options: "+invalid_options.to_display_text self.uri) else
                    if check_exists && self.exists then Error.throw (File_Error.Already_Exists self) else
                        # Given that the amount of data written may be large and AWS library does not seem to support streaming it directly, we use a temporary file to store the data.
                        tmp_file = File.create_temporary_file "s3-tmp"
                        Panic.with_finalizer tmp_file.delete <|
                            result = tmp_file.with_output_stream [File_Access.Write] action
                            # Only proceed if the write succeeded
                            result.if_not_error <|
                                (translate_file_errors self <| S3.upload_file tmp_file self.bucket self.prefix self.credentials) . if_not_error <|
                                    result


    ## PRIVATE
       ADVANCED
       Creates a new input stream for this file and runs the specified action
       on it.

       Arguments:
       - open_options: A vector of `File_Access` objects determining how to open
         the stream. These options set the access properties of the stream.
       - action: A function that operates on the input stream and returns some
         value. The value is returned from this method.

       The created stream is automatically closed when `action` returns (even
       if it returns exceptionally).
    with_input_stream : Vector File_Access -> (Input_Stream -> Any ! File_Error) -> Any ! S3_Error | Illegal_Argument
    with_input_stream self (open_options : Vector) action = if self.is_directory then Error.throw (Illegal_Argument.Error "S3 folders cannot be opened as a stream." self.uri) else
        if (open_options != [File_Access.Read]) then Error.throw (S3_Error.Error "S3 files can only be opened for reading." self.uri) else
            response_body = translate_file_errors self <| S3.get_object self.bucket self.prefix self.credentials
            response_body.with_stream action

    ## ALIAS load, open
       GROUP Standard.Base.Input
       ICON data_input
       Read a file using the specified file format

       Arguments:
       - format: A `File_Format` object used to read file into memory.
         If `Auto_Detect` is specified; the provided file determines the specific
         type and configures it appropriately. If there is no matching type then
         a `File_Error.Unsupported_Type` error is returned.
       - on_problems: Specifies the behavior when a problem occurs during the
         function.
         By default, a warning is issued, but the operation proceeds.
         If set to `Report_Error`, the operation fails with a dataflow error.
         If set to `Ignore`, the operation proceeds without errors or warnings.
    @format File_Format.default_widget
    read : File_Format -> Problem_Behavior -> Any ! S3_Error
    read self format=Auto_Detect (on_problems=Problem_Behavior.Report_Warning) =
        _ = on_problems
        case format of
            Auto_Detect -> if self.is_directory then format.read self on_problems else
                response = translate_file_errors self <| S3.get_object self.bucket self.prefix self.credentials
                response.decode Auto_Detect
            _ ->
                metadata = File_Format_Metadata.Value path=self.path name=self.name
                self.with_input_stream [File_Access.Read] (stream-> format.read_stream stream metadata)

    ## ALIAS load bytes, open bytes
       ICON data_input
       Reads all bytes in this file into a byte vector.
    read_bytes : Vector ! File_Error
    read_bytes self =
        self.read Bytes

    ## ALIAS load text, open text
       ICON data_input
       Reads the whole file into a `Text`, with specified encoding.

       Arguments:
       - encoding: The text encoding to decode the file with. Defaults to UTF-8.
       - on_problems: Specifies the behavior when a problem occurs during the
         function.
         By default, a warning is issued, but the operation proceeds.
         If set to `Report_Error`, the operation fails with a dataflow error.
         If set to `Ignore`, the operation proceeds without errors or warnings.
    @encoding Encoding.default_widget
    read_text : Encoding -> Problem_Behavior -> Text ! File_Error
    read_text self (encoding=Encoding.utf_8) (on_problems=Problem_Behavior.Report_Warning) =
        self.read (Plain_Text encoding) on_problems

    ## UNSTABLE
       Deletes the object.
    delete : Nothing
    delete self = if self.is_directory then Error.throw (S3_Error.Error "Deleting S3 folders is currently not implemented." self.uri) else
        if Context.Output.is_enabled.not then Error.throw (Forbidden_Operation.Error "Deleting an S3_File is forbidden as the Output context is disabled.") else
            translate_file_errors self <| S3.delete_object self.bucket self.prefix self.credentials . if_not_error <| Nothing

    ## Deletes the file if it had existed.
    delete_if_exists : Nothing
    delete_if_exists self =
        r = self.delete
        r.catch File_Error err-> case err of
            File_Error.Not_Found _ -> Nothing
            _ -> r

    ## Copies the file to the specified destination.

       Arguments:
       - destination: the destination to move the file to.
       - replace_existing: specifies if the operation should proceed if the
         destination file already exists. Defaults to `False`.
    copy_to : Writable_File -> Boolean -> Any ! File_Error
    copy_to self (destination : Writable_File) (replace_existing : Boolean = False) =
        if self.is_directory then Error.throw (S3_Error.Error "Copying S3 folders is currently not implemented." self.uri) else
            if Context.Output.is_enabled.not then Error.throw (Forbidden_Operation.Error "Copying an S3_File is forbidden as the Output context is disabled.") else
                case destination.file of
                    # Special shortcut for more efficient handling of S3 file copying (no need to move the data to our machine)
                    s3_destination : S3_File ->
                        if replace_existing.not && s3_destination.exists then Error.throw (File_Error.Already_Exists destination) else
                            translate_file_errors self <| S3.copy_object self.bucket self.prefix s3_destination.bucket s3_destination.prefix self.credentials . if_not_error <| s3_destination
                    _ -> generic_copy self destination.file replace_existing

    ## Moves the file to the specified destination.

       ! S3 Move is a Copy and Delete

         Since S3 does not support moving files, this operation is implemented
         as a copy followed by delete. Keep in mind that the space usage of the
         file will briefly be doubled and that the operation may not be as fast
         as a local move often is.

       Arguments:
       - destination: the destination to move the file to.
       - replace_existing: specifies if the operation should proceed if the
         destination file already exists. Defaults to `False`.
    move_to : Writable_File -> Boolean -> Nothing ! File_Error
    move_to self (destination : Writable_File) (replace_existing : Boolean = False) =
        if Context.Output.is_enabled.not then Error.throw (Forbidden_Operation.Error "File moving is forbidden as the Output context is disabled.") else
            r = self.copy_to destination replace_existing=replace_existing
            r.if_not_error <|
                self.delete.if_not_error <| r

    ## GROUP Standard.Base.Operators
       Join two path segments together.

       Arguments:
       - subpath: The path to join to the path of `self`.
    / : Text -> S3_File
    / self subpath = if self.is_directory.not then Error.throw (S3_Error.Error "Only folders can have children." self.uri) else
        trimmed = if subpath.starts_with "/" then subpath.drop (First 1) else subpath
        parts = trimmed.split "/"

        extend current part =
            if current == "" then part else
                if current.ends_with "/" then current + part else
                    current + "/" + part

        loop current remaining = if remaining.length == 0 then current else
            new_current = case remaining.first of
                ".." ->
                    last_index = current.last_index_of "/"
                    if last_index == Nothing then (S3_Error.Error "Cannot move above root folder.") else current.take last_index
                "." -> current
                x -> extend current x
            @Tail_Call loop new_current (remaining.drop 1)

        initial = if subpath.starts_with "/" then "" else self.prefix
        path = loop initial parts
        S3_File.Value self.bucket path self.credentials

    ## GROUP Standard.Base.Calculations
       Join two or more path segments together, normalizing the `..` and `.` subpaths.

       Arguments:
       - subpaths: The path segment or segments to join to the path of `self`.
    join : (Text | Vector) -> S3_File
    join self subpaths = case subpaths of
        _ : Vector -> (subpaths.fold self c->p-> c / p)
        _ -> self.join [subpaths]

    ## GROUP Standard.Base.Metadata
       Resolves the parent of this file.
    parent : S3_File | Nothing
    parent self =
        if self.prefix == "" then Nothing else
            is_directory = self.prefix.ends_with "/"
            last_index = case is_directory of
                # For directories we drop the trailing slash and find the one before it:
                True -> (self.prefix.drop (Last 1)).last_index_of "/"
                False -> self.prefix.last_index_of "/"
            ## We include the trailing slash in the path, as the parent is
               always a directory and in S3 directories are distinguished only
               by the presence of this slash.
            new_prefix = if last_index == Nothing then "" else self.prefix.take last_index+1
            S3_File.Value self.bucket new_prefix self.credentials

    ## GROUP Standard.Base.Metadata
       Checks if `self` is a descendant of `other`.
    is_descendant_of : S3_File -> Boolean
    is_descendant_of self other = other.is_directory && self.uri.starts_with other.uri

    ## GROUP Standard.Base.Metadata
       Returns the path of this file.
    path : Text
    path self = self.uri

    ## GROUP Standard.Base.Metadata
       Returns the name of this file.
    name : Text
    name self = if self.prefix == "" then self.bucket else
        trimmed = if self.prefix.ends_with "/" then self.prefix.drop (Last 1) else self.prefix
        last_index = trimmed.last_index_of "/"
        if last_index == Nothing then trimmed else trimmed.drop (First last_index+1)

    ## GROUP Standard.Base.Metadata
       Returns the extension of the file.
    extension : Text
    extension self = if self.is_directory then Error.throw (S3_Error.Error "Directories do not have extensions." self.uri) else
        find_extension_from_name self.name

    ## GROUP Standard.Base.Metadata
       Gets the creation time of a file.
    creation_time : Date_Time ! File_Error
    creation_time self =
        Error.throw (S3_Error.Error "Creation time is not available for S3 files, consider using `last_modified_time` instead." self.uri)

    ## GROUP Standard.Base.Metadata
       Gets the last modified time of a file.
    last_modified_time : Date_Time ! File_Error
    last_modified_time self =
        if self.is_directory then Error.throw (S3_Error.Error "`last_modified_time` can only be called on files." self.uri) else
            instant = translate_file_errors self <| S3.raw_head self.bucket self.prefix self.credentials . lastModified
            if instant.is_nothing then Error.throw (S3_Error.Error "Missing information for: lastModified" self.uri) else
                instant.at_zone Time_Zone.system

    ## GROUP Standard.Base.Input
       Lists files contained in the directory denoted by this file.

       Arguments:
       - name_filter: A glob pattern that can be used to filter the returned
         files. If it is not specified, all files are returned.
       - recursive: Specifies whether the returned list of files should include
         also files from the subdirectories. If set to `False` (the default),
         only the immediate children of the listed directory are considered.

       The `name_filter` can contain the following special characters:

       If `recursive` is set to True and a `name_filter` does not contain `**`,
       it will be automatically prefixed with `**/` to allow matching files in
       subdirectories.
    list : Text -> Boolean -> Vector S3_File
    list self name_filter:Text="" recursive:Boolean=False =
        check_name_filter action = if name_filter != "" then Unimplemented.throw "S3 listing with name filter is not currently implemented." else action
        check_recursion action = if recursive then Unimplemented.throw "S3 listing with recursion is not currently implemented." else action
        check_directory action = if self.is_directory.not then Error.throw (S3_Error.Error "Only folders can have children." self.uri) else action

        check_directory <| check_recursion <| check_name_filter <|
            if self.bucket == "" then translate_file_errors self <| S3.list_buckets self.credentials . map bucket-> S3_File.Value bucket "" self.credentials else
                pair = translate_file_errors self <| S3.read_bucket self.bucket self.prefix self.credentials
                sub_folders = pair.first . map key-> S3_File.Value self.bucket key self.credentials
                files = pair.second . map key-> S3_File.Value self.bucket key self.credentials
                sub_folders + files

## PRIVATE
File_Format_Metadata.from (that : S3_File) = File_Format_Metadata.Value that.uri that.name (that.extension.catch _->Nothing)

## PRIVATE
File_Like.from (that : S3_File) = File_Like.Value that

## PRIVATE
Writable_File.from (that : S3_File) =
    Writable_File.Value that S3_File_Write_Strategy.instance

## PRIVATE
   A helper that translates lower level S3 errors to file-system errors.
translate_file_errors related_file result =
    result.catch S3_Key_Not_Found error->
        s3_file = S3_File.Value error.bucket error.key related_file.credentials
        Error.throw (File_Error.Not_Found s3_file)
