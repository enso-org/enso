from Standard.Base import all
import Standard.Base.Enso_Cloud.Data_Link.Data_Link
import Standard.Base.Enso_Cloud.Data_Link_Helpers
import Standard.Base.Errors.Common.Syntax_Error
import Standard.Base.Errors.File_Error.File_Error
import Standard.Base.Errors.Illegal_Argument.Illegal_Argument
import Standard.Base.Errors.Unimplemented.Unimplemented
import Standard.Base.Runtime.Context
import Standard.Base.System.File.Data_Link_Access.Data_Link_Access
import Standard.Base.System.File.Generic.File_Like.File_Like
import Standard.Base.System.File.Generic.Writable_File.Writable_File
import Standard.Base.System.File_Format_Metadata.File_Format_Metadata
import Standard.Base.System.Input_Stream.Input_Stream
import Standard.Base.System.Output_Stream.Output_Stream
from Standard.Base.System.File import find_extension_from_name
from Standard.Base.System.File.Generic.File_Write_Strategy import generic_copy

import project.AWS_Credential.AWS_Credential
import project.Errors.S3_Bucket_Not_Found
import project.Errors.S3_Error
import project.Errors.S3_Key_Not_Found
import project.Internal.S3_File_Write_Strategy
import project.Internal.S3_Path.S3_Path
import project.S3.S3

## Represents an S3 file or folder
   If the path ends with a slash, it is a folder. Otherwise, it is a file.
type S3_File
    ## ICON data_input

       Given an S3 URI, create a file representation.

       Arguments:
       - uri: The URI of the file.
         The URI must be in the form `s3://bucket/path/to/file`.
         If the path contains `.` or `..` segments, they will be normalized.
       - credentials: The credentials to use when accessing the file.
         If not specified, the default credentials are used.
         Note, the credentials are not verified until the file is accessed.

       Returns:
       - An `S3_File` object representing the file.

       ! Error Conditions
       - If the URI is not in the correct format, an `Illegal_Argument` error is
         thrown.
    new : Text -> AWS_Credential -> S3_File ! Illegal_Argument
    new (uri : Text = S3.uri_prefix) credentials:AWS_Credential=..Default =
        S3_File.Value (S3_Path.parse uri) credentials

    ## PRIVATE
    private Value (s3_path : S3_Path) credentials:AWS_Credential

    ## PRIVATE
       ADVANCED
       Creates a new output stream for this file and runs the specified action
       on it.

       The created stream is automatically closed when `action` returns (even
       if it returns exceptionally).

       Arguments:
       - open_options: A vector of `File_Access` objects determining how to open
         the stream. These options set the access properties of the stream.
       - action: A function that operates on the output stream and returns some
         value. The value is returned from this method.
    with_output_stream : Vector File_Access -> (Output_Stream -> Any ! File_Error) -> Any ! File_Error
    with_output_stream self (open_options : Vector) action = if self.is_directory then Error.throw (S3_Error.Error "S3 directory cannot be opened as a stream." self.uri) else
        Context.Output.if_enabled disabled_message="Writing to an S3_File is forbidden as the Output context is disabled." panic=False <|
            open_as_data_link = (open_options.contains Data_Link_Access.No_Follow . not) && (Data_Link.is_data_link self)
            if open_as_data_link then Data_Link_Helpers.write_data_link_as_stream self open_options action else
                if open_options.contains File_Access.Append then Error.throw (S3_Error.Error "S3 does not support appending to a file. Instead you may read it, modify and then write the new contents." self.uri) else
                    File_Access.ensure_only_allowed_options "with_output_stream" [File_Access.Write, File_Access.Create_New, File_Access.Truncate_Existing, File_Access.Create, Data_Link_Access.No_Follow] open_options <|
                        # The exists check is not atomic, but it is the best we can do with S3
                        check_exists = open_options.contains File_Access.Create_New
                        if check_exists && self.exists then Error.throw (File_Error.Already_Exists self) else
                            # Given that the amount of data written may be large and AWS library does not seem to support streaming it directly, we use a temporary file to store the data.
                            tmp_file = File.create_temporary_file "s3-tmp"
                            Panic.with_finalizer tmp_file.delete <|
                                result = tmp_file.with_output_stream [File_Access.Write] action
                                # Only proceed if the write succeeded
                                result.if_not_error <|
                                    (translate_file_errors self <| S3.upload_file tmp_file self.s3_path.bucket self.s3_path.key self.credentials) . if_not_error <|
                                        result


    ## PRIVATE
       ADVANCED
       Creates a new input stream for this file and runs the specified action
       on it.

       The created stream is automatically closed when `action` returns (even
       if it returns exceptionally).

       Arguments:
       - open_options: A vector of `File_Access` objects determining how to open
         the stream. These options set the access properties of the stream.
       - action: A function that operates on the input stream and returns some
         value. The value is returned from this method.
    with_input_stream : Vector File_Access -> (Input_Stream -> Any ! File_Error) -> Any ! S3_Error | Illegal_Argument
    with_input_stream self (open_options : Vector) action = if self.is_directory then Error.throw (Illegal_Argument.Error "S3 folders cannot be opened as a stream." self.uri) else
        open_as_data_link = (open_options.contains Data_Link_Access.No_Follow . not) && (Data_Link.is_data_link self)
        if open_as_data_link then Data_Link_Helpers.read_data_link_as_stream self open_options action else
            File_Access.ensure_only_allowed_options "with_input_stream" [File_Access.Read, Data_Link_Access.No_Follow] open_options <|
                response_body = translate_file_errors self <| S3.get_object self.s3_path.bucket self.s3_path.key self.credentials delimiter=S3_Path.delimiter
                response_body.with_stream action

    ## ALIAS load, open
       GROUP Standard.Base.Input
       ICON data_input

       Read a file using the specified file format

       Arguments:
       - format: A `File_Format` object used to read file into memory.
         If `Auto_Detect` is specified; the provided file determines the specific
         type and configures it appropriately.
       - on_problems: Specifies the behavior when a problem occurs during the
         function.
         By default, a warning is issued, but the operation proceeds.
         If set to `Report_Error`, the operation fails with a dataflow error.
         If set to `Ignore`, the operation proceeds without errors or warnings.

       Returns:
       - The contents of the file read using the specified `File_Format`.

       ! Error Conditions
       - If the bucket does not exist, an `S3_Bucket_Not_Found` error is thrown.
       - If the target is a directory, an `Illegal_Argument` error is thrown.
       - If the file does not exist, a `File_Error.Not_Found` error is thrown.
       - If the access to the object is forbidden, an `S3_Error.Access_Denied`
         error is thrown.
       - If using `Auto_Detect`, and there is no matching type then a
         `File_Error.Unsupported_Type` error is returned.
    @format File_Format.default_widget
    read : File_Format -> Problem_Behavior -> Any ! S3_Error
    read self format=Auto_Detect (on_problems : Problem_Behavior = ..Report_Warning) =
        if self.is_directory then Error.throw (Illegal_Argument.Error "Cannot `read` a directory, use `list`.") else
            if Data_Link.is_data_link self then Data_Link_Helpers.read_data_link self format on_problems else
                case format of
                    Auto_Detect ->
                        response = translate_file_errors self <| S3.get_object self.s3_path.bucket self.s3_path.key self.credentials delimiter=S3_Path.delimiter
                        response.decode Auto_Detect
                    _ ->
                        metadata = File_Format_Metadata.Value path=self.path name=self.name
                        resolved_format = File_Format.resolve format
                        self.with_input_stream [File_Access.Read] (stream-> resolved_format.read_stream stream metadata)

    ## GROUP Standard.Base.Input
       ICON data_input

       Lists files contained in the directory denoted by this file.
       Note, as S3 does not have a native notion of directories, this operation
       will return an empty Vector if the folder does not exist.

       Arguments:
       - name_filter: A glob pattern that can be used to filter the returned
         files. If it is not specified, all files are returned.
       - recursive: Specifies whether the returned list of files should include
         also files from the subdirectories. If set to `False` (the default),
         only the immediate children of the listed directory are considered.

       Returns:
       - A vector of `S3_File` objects representing the files in the directory.

       ! Error Conditions
       - If the bucket does not exist, an `S3_Bucket_Not_Found` error is thrown.
       - If the target is a file, an `Illegal_Argument` error is thrown.
       - If the access to the object is forbidden, an `S3_Error.Access_Denied`
         error is thrown.
       - If using `name_filter` or `recursive`, an `Unimplemented` error is
         thrown as these are currently unsupported.
    list : Text -> Boolean -> Vector S3_File
    list self name_filter:Text="" recursive:Boolean=False =
        check_name_filter action = if name_filter != "" then Unimplemented.throw "S3 listing with name filter is not currently implemented." else action
        check_recursion action = if recursive then Unimplemented.throw "S3 listing with recursion is not currently implemented." else action
        check_directory action = if self.is_directory.not then Error.throw (Illegal_Argument.Error "Cannot `list` a non-directory." self.uri) else action

        check_directory <| check_recursion <| check_name_filter <|
            if self.s3_path.bucket == "" then translate_file_errors self <| S3.list_buckets self.credentials . map bucket-> S3_File.Value (S3_Path.Value bucket "") self.credentials else
                pair = translate_file_errors self <| S3.read_bucket self.s3_path.bucket self.s3_path.key self.credentials delimiter=S3_Path.delimiter
                bucket = self.s3_path.bucket
                sub_folders = pair.first . map key->
                    S3_File.Value (S3_Path.Value bucket key) self.credentials
                files = pair.second . map key->
                    S3_File.Value (S3_Path.Value bucket key) self.credentials
                sub_folders + files

    ## ALIAS load bytes, open bytes
       ICON data_input
       Reads all bytes in this file into a byte vector.

       Returns:
       - The contents of the file as a vector of bytes.

       ! Error Conditions
       - If the bucket does not exist, an `S3_Bucket_Not_Found` error is thrown.
       - If the target is a directory, an `Illegal_Argument` error is thrown.
       - If the file does not exist, a `File_Error.Not_Found` error is thrown.
       - If the access to the object is forbidden, an `S3_Error.Access_Denied`
         error is thrown.
    read_bytes : Vector ! File_Error
    read_bytes self = if self.is_directory then Error.throw (Illegal_Argument.Error "Cannot `read_bytes` of a directory.") else
        self.read Bytes

    ## ALIAS load text, open text
       ICON data_input
       Reads the whole file into a `Text`, with specified encoding.

       Arguments:
       - encoding: The text encoding to decode the file with. Defaults to UTF-8.
       - on_problems: Specifies the behavior when a problem occurs during the
         function.

       Returns:
       - The contents of the file as `Text` decoded with the specified encoding.

       ! Error Conditions
       - If the bucket does not exist, an `S3_Bucket_Not_Found` error is thrown.
       - If the target is a directory, an `Illegal_Argument` error is thrown.
       - If the file does not exist, a `File_Error.Not_Found` error is thrown.
       - If the access to the object is forbidden, an `S3_Error.Access_Denied`
         error is thrown.
       - If there is a problem decoding the byte stream to text, then an
         `Encoding_Error` will be raised following the `on_problems` behavior.
    @encoding Encoding.default_widget
    read_text : Encoding -> Problem_Behavior -> Text ! File_Error
    read_text self (encoding : Encoding = Encoding.default) (on_problems : Problem_Behavior = ..Report_Warning) =
        if self.is_directory then Error.throw (Illegal_Argument.Error "Cannot `read_text` of a directory.") else
            self.read (Plain_Text_Format.Plain_Text encoding) on_problems

    ## ICON data_output
       Copies the S3 file to the specified destination.

       Arguments:
       - destination: the destination to copy the file to.
       - replace_existing: specifies if the operation should proceed if the
         destination file already exists. Defaults to `False`.

       Returns:
       - The destination file if the operation was successful.

       ! Error Conditions
       - Unless `replace_existing` is set to `True`, if the destination file
         already exists, a `File_Error` is thrown.
       - If the source is a directory, an `S3_Error` will occur as this is not
         currently supported.
       - If the Output operations are disabled, a `Forbidden_Operation` panic
         will occur.
    copy_to : File_Like -> Boolean -> Any ! S3_Error | File_Error
    copy_to self (destination : File_Like) (replace_existing : Boolean = False) = Data_Link_Helpers.disallow_links_in_copy self destination <|
        if self.is_directory then Error.throw (S3_Error.Error "Copying S3 folders is currently not implemented." self.uri) else
            Context.Output.if_enabled disabled_message="Copying an S3_File is forbidden as the Output context is disabled." panic=False <|
                destination_writable = Writable_File.from destination
                case destination_writable.file of
                    # Special shortcut for more efficient handling of S3 file copying (no need to move the data to our machine)
                    s3_destination : S3_File ->
                        if s3_destination == self then (if self.exists then self else Error.throw (File_Error.Not_Found self)) else
                            if replace_existing.not && s3_destination.exists then Error.throw (File_Error.Already_Exists destination) else
                                destination_path = s3_destination.s3_path
                                translate_file_errors self <| S3.copy_object self.s3_path.bucket self.s3_path.key destination_path.bucket destination_path.key self.credentials
                                    . if_not_error destination_writable.file_for_return
                    _ -> generic_copy self destination_writable replace_existing

    ## ICON data_output
       Moves the file to the specified destination.

       Arguments:
       - destination: the destination to move the file to.
       - replace_existing: specifies if the operation should proceed if the
         destination file already exists. Defaults to `False`.

       Returns:
       - The destination file if the operation was successful.

       ! Error Conditions
       - Unless `replace_existing` is set to `True`, if the destination file
         already exists, a `File_Error` is thrown.
       - If the source is a directory, an `S3_Error` will occur as this is not
         currently supported.
       - If the Output operations are disabled, a `Forbidden_Operation` panic
         will occur.

       ! S3 Move is a Copy and Delete

         Since S3 does not support moving files, this operation is implemented
         as a copy followed by delete. Keep in mind that the space usage of the
         file will briefly be doubled and that the operation may not be as fast
         as a local move often is.
    move_to : File_Like -> Boolean -> Any ! File_Error
    move_to self (destination : File_Like) (replace_existing : Boolean = False) =
        if self.is_directory then Error.throw (S3_Error.Error "Moving S3 folders is currently not implemented." self.uri) else
            Data_Link_Helpers.disallow_links_in_move self destination <|
                Context.Output.if_enabled disabled_message="File moving is forbidden as the Output context is disabled." panic=False <|
                    r = self.copy_to destination replace_existing=replace_existing
                    r.if_not_error <|
                        # If source and destination are the same, we do not want to delete the file
                        if destination.underlying == self then r else
                            self.delete.if_not_error r

    ## ICON data_output
       Deletes the object.

       Arguments:
       - recursive: If the target is a non-empty directory, it will only be
         removed if this is set to `True`. Defaults to `False`, meaning that the
         operation will fail if the directory is not empty. This option has no
         effect for files or data links.

       ! Error Conditions
       - If the target is a directory and `recursive` is `False`, a
         `File_Error.Directory_Not_Empty` error is thrown.
       - If the bucket or file does not exist, a `File_Error.Not_Found` error is
         thrown.
       - If the access to the object is forbidden, an `S3_Error.Access_Denied`
         error is thrown.
       - If the Output operations are disabled, a `Forbidden_Operation` panic
         will occur.
    delete : Boolean -> Nothing
    delete self (recursive : Boolean = False) =
        if self.exists.not then Error.throw (File_Error.Not_Found self) else
            self.delete_if_exists recursive

    ## ICON data_output
       Deletes the file if it had existed.

       Arguments:
       - recursive: If the target is a non-empty directory, it will only be
         removed if this is set to `True`. Defaults to `False`, meaning that the
         operation will fail if the directory is not empty. This option has no
         effect for files or data links.

       ! Error Conditions
       - If the target is a directory and `recursive` is `False`, a
         `File_Error.Directory_Not_Empty` error is thrown.
       - If the bucket does not exist, an `S3_Bucket_Not_Found` error is thrown.
       - If the access to the object is forbidden, an `S3_Error.Access_Denied`
         error is thrown.
       - If the Output operations are disabled, a `Forbidden_Operation` panic
         will occur.
    delete_if_exists : Boolean -> Nothing
    delete_if_exists self (recursive : Boolean = False) =
        Context.Output.if_enabled disabled_message="Deleting an S3_File is forbidden as the Output context is disabled." panic=False <|
            case self.is_directory of
                True ->
                    # This is a temporary simplified implementation to ensure cleaning up after tests
                    # TODO improve recursive deletion for S3 folders: https://github.com/enso-org/enso/issues/9704
                    children = self.list
                    if children.is_empty.not && recursive.not then Error.throw (File_Error.Directory_Not_Empty self) else
                        r = children.map child-> child.delete_if_exists recursive
                        r.if_not_error self
                False -> translate_file_errors self <| S3.delete_object self.s3_path.bucket self.s3_path.key self.credentials . if_not_error Nothing

    ## GROUP Output
       ICON folder_add
       Creates the directory represented by this file if it did not exist.
       It also creates parent directories if they did not exist.

       Returns:
       - An `Unimplemented` error as this operation is not currently supported.

       ! S3 Directory Handling

         Note that regular S3 buckets do not have a 'native' notion of
         directories, instead they are emulated using prefixes and a delimiter
         (in Enso, the delimiter is set to "/").

         The trailing slash determines if the given path is treated as a
         directory or as a regular file.
    create_directory : File
    create_directory self =
        ## TODO Add more information about how S3 handles directories.
           https://github.com/enso-org/enso/issues/9704
        Unimplemented.throw "Creating S3 folders is currently not implemented."

    ## GROUP Standard.Base.Operators
       ICON folder
       Join two path segments together, normalizing the `..` and `.` sub-paths.

       Arguments:
       - subpath: The path to join to the path of `self`.

       Returns:
       - An `S3_File` representing the joined and normalised path, with the same
         credentials.

       ! S3 Directory Handling

         Note that regular S3 buckets do not have a 'native' notion of
         directories, instead they are emulated using prefixes and a delimiter
         (in Enso, the delimiter is set to "/").

         The trailing slash determines if the given path is treated as a
         directory or as a regular file.

         However, for ease-of-use, if a path without a trailing slash is used
         with the `/` operator it will be accepted and the sub paths will be
         resolved, even though such a path would not be treated as a directory
         by any other operations.

         See: https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-folders.html
    / : Text -> S3_File
    / self subpath =
        S3_File.Value (self.s3_path.resolve subpath) self.credentials

    ## GROUP Standard.Base.Calculations
       ICON folder
       Join two or more path segments together, normalizing the `..` and `.` subpaths.

       Arguments:
       - sub-paths: The path segment or segments to join to the path of `self`.

       Returns:
       - An `S3_File` representing the joined and normalised path, with the same
         credentials.

       ! S3 Directory Handling

         Note that regular S3 buckets do not have a 'native' notion of
         directories, instead they are emulated using prefixes and a delimiter
         (in Enso, the delimiter is set to "/").

         The trailing slash determines if the given path is treated as a
         directory or as a regular file.

         However, for ease-of-use, if a path without a trailing slash is used
         with the `/` operator it will be accepted and the sub paths will be
         resolved, even though such a path would not be treated as a directory
         by any other operations.

         See: https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-folders.html
    join : (Vector | Text) -> S3_File
    join self (subpaths : Vector | Text) =
        vec = Vector.unify_vector_or_element subpaths
        vec_as_texts = vec.map subpath-> (subpath : Text)
        S3_File.Value (self.s3_path.join vec_as_texts) self.credentials

    ## GROUP Standard.Base.Metadata
       ICON metadata

       Returns the name of this file.
    name : Text
    name self = self.s3_path.file_name

    ## GROUP Standard.Base.Metadata
       ICON metadata

       Returns the extension of the file.
    extension : Text
    extension self = if self.is_directory then Error.throw (S3_Error.Error "Directories do not have extensions." self.uri) else
        find_extension_from_name self.name

    ## GROUP Standard.Base.Metadata
       ICON metadata

       Returns the path of this file.
    path : Text
    path self = self.s3_path.to_text

    ## GROUP Standard.Base.Metadata
       ICON metadata

       Returns the URI of this file
       The URI is in the form `s3://bucket/path/to/file`.
    uri : Text
    uri self -> Text = self.s3_path.to_text

    ## GROUP Standard.Base.Metadata
       ICON metadata

       Checks if the folder or file exists

       Returns:
       - `True` if the bucket or object exists, `False` otherwise.

       ! Error Conditions
       - If the credential is invalid, an `AWS_SDK_Error` is thrown.
       - If access is denied to the bucket, an `S3_Error` is thrown.
    exists : Boolean
    exists self = if self.s3_path.bucket == "" then True else
        raw_result = if self.s3_path.is_root then translate_file_errors self <| S3.head self.s3_path.bucket "" self.credentials . is_error . not else
            pair = translate_file_errors self <| S3.read_bucket self.s3_path.bucket self.s3_path.key self.credentials delimiter=S3_Path.delimiter max_count=1
            pair.second.contains self.s3_path.key
        raw_result.catch S3_Bucket_Not_Found _->False

    ## GROUP Standard.Base.Metadata
       ICON metadata

       Returns the size of a file in bytes.

       ! Error Conditions
       - If the `S3_File` represents a directory, an `S3_Error` error is thrown.
       - If the bucket or object does not exist, a `File_Error.Not_Found` is
         thrown.
       - If the object is not accessible, an `S3_Error` is thrown.
    size : Integer
    size self =
        if self.is_directory then Error.throw (S3_Error.Error "size can only be called on files." self.uri) else
            content_length = translate_file_errors self <| S3.raw_head self.s3_path.bucket self.s3_path.key self.credentials . contentLength
            if content_length.is_nothing then Error.throw (S3_Error.Error "ContentLength header is missing." self.uri) else content_length

    ## GROUP Standard.Base.Metadata
       ICON metadata

       Gets the creation time of a file.

       Returns:
       - An `S3_Error` error as only the last modified time is available for S3
         objects.
    creation_time : Date_Time ! File_Error
    creation_time self =
        Error.throw (S3_Error.Error "Creation time is not available for S3 files, consider using `last_modified_time` instead." self.uri)

    ## GROUP Standard.Base.Metadata
       ICON metadata

       Returns the last modified time of a file.

       ! Error Conditions
       - If the `S3_File` represents a directory, an `S3_Error` error is thrown.
       - If the bucket or object does not exist, a `File_Error.Not_Found` is
         thrown.
       - If the object is not accessible, an `S3_Error` is thrown.
    last_modified_time : Date_Time ! File_Error
    last_modified_time self =
        if self.is_directory then Error.throw (S3_Error.Error "`last_modified_time` can only be called on files." self.uri) else
            instant = translate_file_errors self <| S3.raw_head self.s3_path.bucket self.s3_path.key self.credentials . lastModified
            if instant.is_nothing then Error.throw (S3_Error.Error "Missing information for: lastModified" self.uri) else
                instant.at_zone Time_Zone.system

    ## GROUP Standard.Base.Metadata
       ICON metadata
       Checks if this is a folder.

       Returns:
       - `True` if the S3 path represents a folder, `False` otherwise.

       ! S3 Directory Handling

         Note that regular S3 buckets do not have a 'native' notion of
         directories, instead they are emulated using prefixes and a delimiter
         (in Enso, the delimiter is set to "/").

         The trailing slash determines if the given path is treated as a
         directory or as a regular file.
    is_directory : Boolean
    is_directory self = self.s3_path.is_directory

    ## GROUP Standard.Base.Metadata
       ICON metadata
       Checks if this is a regular file.

       Returns:
       - `True` if the S3 path represents a file, `False` otherwise.

       ! S3 Directory Handling

         Note that regular S3 buckets do not have a 'native' notion of
         directories, instead they are emulated using prefixes and a delimiter
         (in Enso, the delimiter is set to "/").

         The trailing slash determines if the given path is treated as a
         directory or as a regular file.
    is_regular_file : Boolean
    is_regular_file self = self.is_directory.not

    ## GROUP Standard.Base.Metadata
       ICON metadata

       Resolves the parent of this file.

       Returns:
       - The parent of this file as an `S3_File` object or if a top level then
         `Nothing`.
    parent : S3_File | Nothing
    parent self =
        parent_path = self.s3_path.parent
        parent_path.if_not_nothing <|
            S3_File.Value parent_path self.credentials

    ## GROUP Standard.Base.Metadata
       ICON metadata

       Checks if `self` is a descendant of `other`.

       Returns:
       - `True` if `self` is a descendant of `other`, `False` otherwise.
    is_descendant_of : S3_File -> Boolean
    is_descendant_of self other = self.s3_path.is_descendant_of other.s3_path

## PRIVATE
File_Format_Metadata.from (that : S3_File) = File_Format_Metadata.Value that.uri that.name (that.extension.catch _->Nothing)

## PRIVATE
File_Like.from (that : S3_File) = File_Like.Value that

## PRIVATE
Writable_File.from (that : S3_File) = if Data_Link.is_data_link that then Data_Link_Helpers.interpret_data_link_as_writable_file that else
    Writable_File.Value that S3_File_Write_Strategy.instance

## PRIVATE
   A helper that translates lower level S3 errors to file-system errors.
translate_file_errors related_file result =
    result.catch S3_Key_Not_Found error->
        s3_path = S3_Path.Value error.bucket error.key
        s3_file = S3_File.Value s3_path related_file.credentials
        Error.throw (File_Error.Not_Found s3_file)
