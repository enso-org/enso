from Standard.Base import all
import Standard.Base.Enso_Cloud.Data_Link.Data_Link
import Standard.Base.Errors.Illegal_Argument.Illegal_Argument
import Standard.Base.Errors.Illegal_State.Illegal_State
import Standard.Base.Runtime.Ref.Ref
from Standard.Base.Enso_Cloud.Data_Link_Helpers import secure_value_to_json

from Standard.Table import Table, Value_Type, Aggregate_Column, Bits, expr
from Standard.Table.Errors import Invalid_Column_Names, Inexact_Type_Coercion, Duplicate_Output_Column_Names

import Standard.Database.DB_Column.DB_Column
import Standard.Database.DB_Table.DB_Table
import Standard.Database.Feature.Feature
import Standard.Database.Dialect_Flag.Dialect_Flag
import Standard.Database.SQL_Type.SQL_Type
import Standard.Database.Internal.Replace_Params.Replace_Params
from Standard.Database import all
from Standard.Database.Errors import all

from Standard.Snowflake import all

from Standard.Test import all
import Standard.Test.Test_Environment

import enso_dev.Table_Tests
import enso_dev.Table_Tests.Database.Common.Audit_Spec
import enso_dev.Table_Tests.Database.Common.Common_Spec
import enso_dev.Table_Tests.Database.Common.IR_Spec
import enso_dev.Table_Tests.Database.Common.Save_Connection_Data_Link
import enso_dev.Table_Tests.Database.Transaction_Spec
import enso_dev.Table_Tests.Database.Upload_Spec
import enso_dev.Table_Tests.Database.Helpers.Name_Generator
import enso_dev.Table_Tests.Common_Table_Operations
from enso_dev.Table_Tests.Common_Table_Operations.Util import all

import enso_dev.Base_Tests.Network.Enso_Cloud.Cloud_Tests_Setup.Cloud_Tests_Setup

type Snowflake_Info_Data
    Value ~data

    connection self = self.data.at 0
    tinfo self = self.data.at 1
    t self = self.data.at 2

    setup default_connection = Snowflake_Info_Data.Value <|
        connection = default_connection.get
        tinfo = Name_Generator.random_name "Tinfo"
        connection.execute_update 'CREATE TEMPORARY TABLE "'+tinfo+'" ("strs" VARCHAR, "ints" NUMBER(38,0), "bools" BOOLEAN, "doubles" FLOAT8)'
        t = connection.query (SQL_Query.Table_Name tinfo)
        row1 = ["a", Nothing, False, 1.2]
        row2 = ["abc", Nothing, Nothing, 1.3]
        row3 = ["def", 42, True, 1.4]
        Panic.rethrow <|
            t.update_rows (Table.from_rows ["strs", "ints", "bools", "doubles"] [row1, row2, row3]) update_action=Update_Action.Insert
        [connection, tinfo, t]

    teardown self =
        self.connection.execute_update 'DROP TABLE "'+self.tinfo+'"'

type Tables_And_Views_Data
    Value ~data

    connection self = self.data.at 0
    tinfo self = self.data.at 1
    vinfo self = self.data.at 2
    temporary_table self = self.data.at 3

    setup default_connection = Tables_And_Views_Data.Value <|
        connection = default_connection.get
        tinfo = Name_Generator.random_name "TestTable"
        connection.execute_update 'CREATE TABLE "'+tinfo+'" ("A" VARCHAR)'

        vinfo = Name_Generator.random_name "TestView"
        connection.execute_update 'CREATE VIEW "'+vinfo+'" AS SELECT "A" FROM "'+tinfo+'";'

        temporary_table = Name_Generator.random_name "TemporaryTable"
        (Table.new [["X", [1, 2, 3]]]).select_into_database_table connection temporary_table temporary=True
        [connection, tinfo, vinfo, temporary_table]

    teardown self =
        self.connection.execute_update 'DROP VIEW "'+self.vinfo+'";'
        self.connection.execute_update 'DROP TABLE "'+self.tinfo+'";'

type Snowflake_Aggregate_Data
    Value ~data

    connection self = self.data.at 0
    name self = self.data.at 1
    t self = self.data.at 2

    setup default_connection = Snowflake_Aggregate_Data.Value <|
        connection = default_connection.get
        name = Name_Generator.random_name "Ttypes"
        connection.execute_update 'CREATE TEMPORARY TABLE "'+name+'" ("txt" VARCHAR, "i1" SMALLINT, "i2" INT, "i3" BIGINT, "i4" NUMERIC, "r1" REAL, "r2" DOUBLE PRECISION, "bools" BOOLEAN)' . if_not_error <|
            t = connection.query (SQL_Query.Table_Name name)
            [connection, name, t]

    teardown self =
        self.connection.execute_update 'DROP TABLE "'+self.name+'"'

snowflake_specific_spec suite_builder default_connection db_name setup =
    table_builder = setup.table_builder
    light_table_builder = setup.light_table_builder
    materialize = setup.materialize

    suite_builder.group "[Snowflake] Schemas and Databases" group_builder->
        group_builder.specify "should be able to get current database and list databases" <|
            connection = default_connection.get
            connection.database.equals_ignore_case db_name . should_be_true
            connection.databases.length . should_not_equal 0
            connection.databases.find (name-> name.equals_ignore_case db_name) . should_succeed
            Meta.is_same_object connection (connection.set_database db_name) . should_be_true

        group_builder.specify "should be able to get current schema and list schemas" <|
            connection = default_connection.get
            connection.schema.equals_ignore_case "public" . should_be_true
            connection.schemas.length . should_not_equal 0
            connection.schemas.find (name-> name.equals_ignore_case "public") . should_succeed
            Meta.is_same_object connection (connection.set_schema "public") . should_be_true

        group_builder.specify "should allow changing schema" <|
            connection = default_connection.get
            new_connection = connection.set_schema "information_schema"
            new_schema = new_connection.read (SQL_Query.Raw_SQL "SELECT current_schema()") . at 0 . to_vector . first
            new_schema . should_equal "INFORMATION_SCHEMA"

        group_builder.specify "should allow changing database" <|
            connection = default_connection.get
            databases = connection.databases.filter d->((d!=db_name) && (d!='rdsadmin'))
            pending_database = if databases.length != 0 then Nothing else "Cannot test changing database unless two databases defined."
            case pending_database of
                Nothing ->
                    new_connection = connection.set_database databases.first
                    new_database = new_connection.read (SQL_Query.Raw_SQL "SELECT current_database()") . at 0 . to_vector . first
                    new_database . should_equal databases.first
                # Nop - skip the test
                _ -> Nothing

    suite_builder.group "[Snowflake] Tables and Table Types" group_builder->
        data = Tables_And_Views_Data.setup default_connection

        group_builder.teardown <|
            data.teardown

        group_builder.specify "should be able to list table types" <|
            table_types = data.connection.table_types
            table_types.length . should_not_equal 0
            table_types.contains "TABLE" . should_be_true
            table_types.contains "VIEW" . should_be_true

        group_builder.specify "should be able to list tables" <|
            tables = data.connection.tables
            tables.row_count . should_not_equal 0
            tables.columns.map .name . should_equal ["Database", "Schema", "Name", "Type", "Description"]

            table_names = tables.at "Name" . to_vector
            table_names.should_contain data.tinfo
            table_names.should_contain data.vinfo
            table_names.should_contain data.temporary_table

        group_builder.specify "should be able to filter tables by name" <|
            tables = data.connection.tables data.tinfo
            tables.row_count . should_equal 1
            tables.at "Database" . to_vector . at 0 . equals_ignore_case db_name . should_be_true
            tables.at "Schema" . to_vector . at 0 . equals_ignore_case "public" . should_be_true
            tables.at "Name" . to_vector . at 0 . should_equal data.tinfo
            tables.at "Type" . to_vector . at 0 . should_equal "TABLE"

            data.connection.tables "TestT_ble%" . row_count . should_equal 1
            data.connection.tables "Temporary%ble%" . row_count . should_equal 1
            data.connection.tables "Temporary%ble%" . at "Type" . to_vector . should_equal ["TEMPORARY"]
            data.connection.tables "N_nexistent%" . row_count . should_equal 0

        group_builder.specify "should be able to filter tables by type" <|
            tables = data.connection.tables types=["VIEW"]
            tables.row_count . should_not_equal 0
            tables.at "Name" . to_vector . contains data.tinfo . should_be_false
            tables.at "Name" . to_vector . contains data.vinfo . should_be_true


    suite_builder.group "[Snowflake] Info" group_builder->
        data = Snowflake_Info_Data.setup default_connection

        group_builder.teardown <|
            data.teardown

        group_builder.specify "should return Table information" <|
            i = data.t.column_info
            i.at "Column" . to_vector . should_equal ["strs", "ints", "bools", "doubles"]
            i.at "Items Count" . to_vector . should_equal [3, 1, 2, 3]
            # The integer column is treated as NUMBER(38, 0) in Snowflake so the value type reflects that:
            i.at "Value Type" . to_vector . should_equal [Value_Type.Char, Value_Type.Decimal 38 0, Value_Type.Boolean, Value_Type.Float]

        group_builder.specify "should return Table information, also for aggregated results" <|
            i = data.t.aggregate columns=[Aggregate_Column.Concatenate "strs", Aggregate_Column.Sum "ints", Aggregate_Column.Count_Distinct "bools"] . column_info
            i.at "Column" . to_vector . should_equal ["Concatenate strs", "Sum ints", "Count Distinct bools"]
            i.at "Items Count" . to_vector . should_equal [1, 1, 1]
            i.at "Value Type" . to_vector . should_equal [Value_Type.Char, Value_Type.Decimal 38 0, Value_Type.Decimal 18 0]

        group_builder.specify "should infer standard types correctly" <|
            data.t.at "strs" . value_type . is_text . should_be_true
            setup.expect_integer_type <| data.t.at "ints"
            data.t.at "bools" . value_type . is_boolean . should_be_true
            data.t.at "doubles" . value_type . is_floating_point . should_be_true

        group_builder.specify "will report true integer types but infer smartly when materialized (small numbers become Integer in-memory, not Decimal)" <|
            t1 = table_builder [["small_ints", [1, 2, 3]]]

            # Integer types are NUMBER(38, 0) in Snowflake so they are all mapped to decimal
            t1.at "small_ints" . value_type . should_equal (Value_Type.Decimal 38 0)
            # The fact that Integer is coerced to Decimal is an expected thing in Snowflake, so we don't warn about this.
            Problems.assume_no_problems t1

            in_memory1 = t1.read
            # But when read back to in-memory, they are inferred as Integer type to avoid the BigInteger overhead
            in_memory1.at "small_ints" . value_type . should_equal (Value_Type.Integer Bits.Bits_64)

            # Again, when materialized the conversion Decimal->Integer is a feature, so it should not cause warning.
            Problems.assume_no_problems in_memory1
            in_memory1.at "small_ints" . to_vector . should_equal_ignoring_order [1, 2, 3]

            t2 = table_builder [["big_ints", [2^100, 2^110, 1]]]
            t2.at "big_ints" . value_type . should_equal (Value_Type.Decimal 38 0)
            # For the decimal column we get a warning because the type changed:
            w = Problems.expect_only_warning Inexact_Type_Coercion t2
            w.requested_type . should_equal (Value_Type.Decimal Nothing 0)
            w.actual_type . should_equal (Value_Type.Decimal 38 0)

            in_memory2 = t2.remove_warnings.read
            # Unless the values are actually big, then the Decimal type is kept, but its precision is lost, as in-memory BigInteger does not store it.
            in_memory2.at "big_ints" . value_type . should_equal (Value_Type.Decimal Nothing 0)
            # The Decimal type loses 'precision' but that is no reason to warn, so we should not see any warnings here:
            Problems.assume_no_problems in_memory2

            # Check correctness of values
            in_memory2.at "big_ints" . to_vector . should_equal_ignoring_order [2^100, 2^110, 1]

        group_builder.specify "correctly handles Decimal and Float types" <|
            table_name = Name_Generator.random_name "DecimalFloat"
            t1 = default_connection.get.create_table table_name [Column_Description.Value "d1" (Value_Type.Decimal 38 6), Column_Description.Value "d2" (Value_Type.Decimal 10 2), Column_Description.Value "d3" (Value_Type.Decimal 24 -3), Column_Description.Value "f" (Value_Type.Float)] primary_key=[] temporary=True
            t1.at "d1" . value_type . should_equal (Value_Type.Decimal 38 6)
            t1.at "d2" . value_type . should_equal (Value_Type.Decimal 10 2)
            # Negative scale is not supported so we fallback to defaults:
            t1.at "d3" . value_type . should_equal (Value_Type.Decimal 38 0)
            t1.at "f" . value_type . should_equal Value_Type.Float

            # We expect warnings about coercing Decimal types
            w1 = Problems.expect_warning Inexact_Type_Coercion t1
            w1.requested_type . should_equal (Value_Type.Decimal 24 -3)
            w1.actual_type . should_equal (Value_Type.Decimal Nothing Nothing)

            t1.update_rows (Table.new [["d1", [1.2345678910]], ["d2", [12.3456]], ["d3", [1234567.8910]], ["f", [1.5]]]) update_action=Update_Action.Insert . should_succeed

            m1 = t1.read
            # Currently in-memory does not support precision and scale in Decimals so they are all change to Nothing
            m1.at "d1" . value_type . should_equal (Value_Type.Decimal Nothing Nothing)
            m1.at "d2" . value_type . should_equal (Value_Type.Decimal Nothing Nothing)
            # The `d3` column got coerced to `Value_Type.Decimal 38 0` so given that the value is relatively small, it is now fetched as integer.
            m1.at "d3" . value_type . should_equal Value_Type.Integer
            m1.at "f" . value_type . should_equal Value_Type.Float

            m1.at "d1" . to_vector . should_equal [Decimal.new "1.234568"]
            m1.at "d2" . to_vector . should_equal [Decimal.new "12.35"]
            m1.at "d3" . to_vector . should_equal [1234568]
            m1.at "f" . to_vector . should_equal [1.5]

    suite_builder.group "[Snowflake] Table.aggregate should correctly infer result types" group_builder->
        data = Snowflake_Aggregate_Data.setup default_connection

        group_builder.teardown <|
            data.teardown

        group_builder.specify "Concatenate, Shortest and Longest" <|
            r = data.t.aggregate columns=[Aggregate_Column.Concatenate "txt", Aggregate_Column.Shortest "txt", Aggregate_Column.Longest "txt"]
            r.columns.at 0 . value_type . should_equal Value_Type.Char
            r.columns.at 1 . value_type . should_equal Value_Type.Char
            r.columns.at 2 . value_type . should_equal Value_Type.Char

        group_builder.specify "Counts" <|
            r = data.t.aggregate columns=[Aggregate_Column.Count, Aggregate_Column.Count_Empty "txt", Aggregate_Column.Count_Not_Empty "txt", Aggregate_Column.Count_Distinct "i1", Aggregate_Column.Count_Not_Nothing "i2", Aggregate_Column.Count_Nothing "i3"]
            r.column_count . should_equal 6

            r.at "Count" . value_type . should_equal (Value_Type.Decimal 18 0)
            r.at "Count Empty txt" . value_type . should_equal (Value_Type.Decimal 13 0)
            r.at "Count Not Empty txt" . value_type . should_equal (Value_Type.Decimal 13 0)
            r.at "Count Distinct i1" . value_type . should_equal (Value_Type.Decimal 18 0)
            r.at "Count Not Nothing i2" . value_type . should_equal (Value_Type.Decimal 18 0)
            r.at "Count Nothing i3" . value_type . should_equal (Value_Type.Decimal 13 0)

        group_builder.specify "Sum" <|
            r = data.t.aggregate columns=[Aggregate_Column.Sum "i1", Aggregate_Column.Sum "i2", Aggregate_Column.Sum "i3", Aggregate_Column.Sum "i4", Aggregate_Column.Sum "r1", Aggregate_Column.Sum "r2"]
            r.columns.at 0 . value_type . should_equal (Value_Type.Decimal 38 0)
            r.columns.at 1 . value_type . should_equal (Value_Type.Decimal 38 0)

            r.columns.at 2 . value_type . should_equal (Value_Type.Decimal 38 0)
            r.columns.at 3 . value_type . should_equal (Value_Type.Decimal 38 0)
            r.columns.at 4 . value_type . should_equal (Value_Type.Float Bits.Bits_64)
            r.columns.at 5 . value_type . should_equal (Value_Type.Float Bits.Bits_64)

        group_builder.specify "Average" <|
            r = data.t.aggregate columns=[Aggregate_Column.Average "i1", Aggregate_Column.Average "i2", Aggregate_Column.Average "i3", Aggregate_Column.Average "i4", Aggregate_Column.Average "r1", Aggregate_Column.Average "r2"]
            r.columns.at 0 . value_type . should_equal (Value_Type.Decimal 38 6)
            r.columns.at 1 . value_type . should_equal (Value_Type.Decimal 38 6)
            r.columns.at 2 . value_type . should_equal (Value_Type.Decimal 38 6)
            r.columns.at 3 . value_type . should_equal (Value_Type.Decimal 38 6)
            r.columns.at 4 . value_type . should_equal Value_Type.Float
            r.columns.at 5 . value_type . should_equal Value_Type.Float


    suite_builder.group "[Snowflake] Warning/Error handling" group_builder->
        group_builder.specify "is capable of handling weird tables" <|
            default_connection.get.execute_update 'CREATE TEMPORARY TABLE "empty-column-name" ("" VARCHAR)' . should_succeed
            t = default_connection.get.query "empty-column-name"
            t.columns.length . should_equal 1
            # The column is renamed to something valid upon read:
            t.column_names . should_equal ["Column 1"]
            # Should be readable:
            t.read . at 0 . to_vector . should_equal []

            Problems.assume_no_problems <|
                default_connection.get.execute_update 'CREATE TEMPORARY TABLE "clashing-unicode-names" ("ś" VARCHAR, "s\u0301" INTEGER)'
            Problems.assume_no_problems <|
                default_connection.get.execute_update 'INSERT INTO "clashing-unicode-names" VALUES (\'A\', 2)'
            t2 = default_connection.get.query (SQL_Query.Table_Name "clashing-unicode-names")
            Problems.expect_only_warning Duplicate_Output_Column_Names t2
            t2.column_names . should_equal ["ś", "ś 1"]
            m2 = t2.read
            m2.at "ś"   . to_vector . should_equal ["A"]
            m2.at "ś 1" . to_vector . should_equal [2]

            r3 = default_connection.get.query (SQL_Query.Raw_SQL 'SELECT 1 AS "A", 2 AS "A"')
            r3.should_fail_with Illegal_Argument
            r3.catch.cause . should_be_a Duplicate_Output_Column_Names

            r4 = default_connection.get.query (SQL_Query.Raw_SQL 'SELECT 1 AS ""')
            r4.should_fail_with Illegal_Argument
            r4.catch.to_display_text . should_contain "The provided custom SQL query is invalid and may suffer data corruption"
            r4.catch.to_display_text . should_contain "The name '' is invalid"

    suite_builder.group "[Snowflake] Edge Cases" group_builder->
        group_builder.specify "materialize should respect the overridden type" <|
            t0 = table_builder [["x", [False, True, False]], ["A", ["a", "b", "c"]], ["B", ["xyz", "abc", "def"]]]
            t1 = t0 . cast "A" (Value_Type.Char size=1) . cast "B" (Value_Type.Char size=3)

            x = t1.at "x"
            a = t1.at "A"
            b = t1.at "B"
            a.value_type.should_equal (Value_Type.Char size=1)
            b.value_type.should_equal (Value_Type.Char size=3)

            c = x.iif a b
            c.to_vector.should_equal_ignoring_order ["xyz", "b", "def"]
            # The max length is lost after the IIF
            c.value_type.should_equal Value_Type.Char

            d = materialize c
            d.to_vector.should_equal_ignoring_order ["xyz", "b", "def"]
            d.value_type.should_equal Value_Type.Char

        group_builder.specify "should be able to round-trip a BigInteger column" <|
            x = 2^70
            m1 = Table.new [["X", [10, x]]]
            m1.at "X" . value_type . should_be_a (Value_Type.Decimal ...)

            t1 = m1.select_into_database_table default_connection.get (Name_Generator.random_name "BigInteger") primary_key=[] temporary=True
            t1.at "X" . value_type . should_equal (Value_Type.Decimal 38 0)
            w1 = Problems.expect_only_warning Inexact_Type_Coercion t1
            w1.requested_type . should_equal (Value_Type.Decimal precision=Nothing scale=0)
            w1.actual_type . should_equal (Value_Type.Decimal precision=38 scale=0)

            v1x = t1.at "X" . to_vector
            v1x.should_equal [10, x]
            v1x.each e-> Test.with_clue "("+e.to_text+"): " <| e.should_be_a Integer

            t2 = t1.set (expr "[X] + 10") "Y"
            t2.at "X" . value_type . should_be_a (Value_Type.Decimal ...)
            t2.at "Y" . value_type . should_be_a (Value_Type.Decimal ...)
            t2.at "X" . to_vector . should_equal_ignoring_order [10, x]

            # Only works by approximation:
            t2.at "Y" . to_vector . should_equal_ignoring_order [20, x+10]
            t2.at "Y" . cast Value_Type.Char . to_vector . should_equal_ignoring_order ["20", (x+10).to_text]

            m2 = t2.remove_warnings.read
            m2.at "X" . value_type . should_be_a (Value_Type.Decimal ...)
            m2.at "Y" . value_type . should_be_a (Value_Type.Decimal ...)
            m2.at "X" . to_vector . should_equal_ignoring_order [10, x]
            m2.at "Y" . to_vector . should_equal_ignoring_order [20, x+10]

            # This has more than 1000 digits.
            super_large = 11^2000
            m3 = Table.new [["X", [super_large]]]
            m3.at "X" . value_type . should_be_a (Value_Type.Decimal ...)
            r3 = m3.select_into_database_table default_connection.get (Name_Generator.random_name "BigInteger2") primary_key=[] temporary=True
            # Snowflake fails to process such a huge number
            r3.should_fail_with SQL_Error
            r3.catch.to_display_text . should_contain "Numeric value"
            r3.catch.to_display_text . should_contain "is not recognized"

        group_builder.specify "should round-trip timestamptz column, preserving instant but converting to UTC" <|
            table_name = Name_Generator.random_name "TimestampTZ"
            table = default_connection.get.create_table table_name [Column_Description.Value "A" (Value_Type.Date_Time with_timezone=True), Column_Description.Value "rowid" Value_Type.Integer] primary_key=[]
            table.should_succeed
            Panic.with_finalizer (default_connection.get.drop_table table.name) <|
                dt1 = Date_Time.new 2022 05 04 15 30 zone=(Time_Zone.utc)
                dt2 = Date_Time.new 2022 05 04 15 30 zone=(Time_Zone.parse "US/Hawaii")
                dt3 = Date_Time.new 2022 05 04 15 30 zone=(Time_Zone.parse "Europe/Warsaw")
                v = [dt1, dt2, dt3]

                Problems.assume_no_problems <|
                    table.update_rows (Table.new [["A", v], ["rowid", (0.up_to v.length).to_vector]]) update_action=Update_Action.Insert

                returned_v =
                    dt1_offset = Date_Time.new 2022 05 04 15 30 zone=(Time_Zone.new 0)
                    dt2_offset = Date_Time.new 2022 05 04 15 30 zone=(Time_Zone.new -10)
                    dt3_offset = Date_Time.new 2022 05 04 15 30 zone=(Time_Zone.new 2)
                    [dt1_offset, dt2_offset, dt3_offset]
                table.sort "rowid" . at "A" . to_vector . should_equal returned_v

                ## We also check how the timestamp column behaves with interpolations:
                v.each my_dt-> Test.with_clue "("+my_dt.to_text+") " <|
                        t2 = table.filter "A" (Filter_Condition.Equal to=my_dt)
                        t2.row_count . should_equal 1

        group_builder.specify "will round-trip timestamp column without timezone by converting it to UTC" <|
            table_name = Name_Generator.random_name "Timestamp"
            table = default_connection.get.create_table table_name [Column_Description.Value "A" (Value_Type.Date_Time with_timezone=False)] primary_key=[]
            Problems.assume_no_problems table
            Panic.with_finalizer (default_connection.get.drop_table table.name) <|
                dt1 = Date_Time.new 2022 05 04 15 30
                dt2 = Date_Time.new 2022 05 04 15 30 zone=(Time_Zone.utc)
                dt3 = Date_Time.new 2022 05 04 15 30 zone=(Time_Zone.parse "US/Hawaii")
                dt4 = Date_Time.new 2022 05 04 15 30 zone=(Time_Zone.parse "Europe/Warsaw")
                v = [dt1, dt2, dt3, dt4]

                source_table = Table.new [["A", v]]
                source_table.at "A" . value_type . should_equal (Value_Type.Date_Time with_timezone=True)
                w = Problems.expect_only_warning Inexact_Type_Coercion <|
                    table.update_rows source_table update_action=Update_Action.Insert
                w.requested_type . should_equal (source_table.at "A" . value_type)
                w.actual_type . should_equal (table.at "A" . value_type)
                w.to_display_text . should_equal "The type Date_Time (with timezone) has been coerced to Date_Time (without timezone). Some information may be lost."

                # When uploading we want to just strip the timezone information and treat every timestamp as LocalDateTime.
                # This is verified by checking the text representation in the DB: it should show the same local time in all 4 cases, regardless of original timezone.
                local_dt = "2022-05-04 15:30:00.000000000"
                table.at "A" . cast Value_Type.Char . to_vector . should_equal [local_dt, local_dt, local_dt, local_dt]

                # Then when downloaded, it should be interpreted at the 'system default' timezone.
                materialized_table = table.read
                materialized_table.at "A" . to_vector . should_equal [dt1, dt1, dt1, dt1]

                # The Inexact_Type_Coercion warning is silenced for this case:
                Problems.assume_no_problems materialized_table

                # We also check how the timestamp column behaves with interpolations:
                # Given that we lost timezone - all entries match.
                v.each my_dt-> Test.with_clue my_dt.to_text+": " <|
                    t2 = table.filter "A" (Filter_Condition.Equal to=my_dt)
                    t2.row_count . should_equal 4
                    t2.at "A" . to_vector . should_equal [dt1, dt1, dt1, dt1]

    suite_builder.group "[Snowflake] math functions" group_builder->
        group_builder.specify "round, trunc, ceil, floor" <|
            col = table_builder [["x", [0.1, 0.9, 3.1, 3.9, -0.1, -0.9, -3.1, -3.9]]] . at "x"
            col . cast Value_Type.Integer . ceil . value_type . should_equal (Value_Type.Decimal 38 0)

            col . cast Value_Type.Float . round . value_type . should_equal Value_Type.Float
            col . cast Value_Type.Integer . round . value_type . should_equal (Value_Type.Decimal 38 0)
            col . cast Value_Type.Decimal . round . value_type . should_equal (Value_Type.Decimal 38 0)

            col . cast Value_Type.Float . round 1 . value_type . should_equal Value_Type.Float
            col . cast Value_Type.Integer . round 1 . value_type . should_equal (Value_Type.Decimal 38 0)
            col . cast Value_Type.Decimal . round 1 . value_type . should_equal (Value_Type.Decimal 38 1)

            col . cast Value_Type.Float . round use_bankers=True . value_type . should_equal Value_Type.Float
            col . cast Value_Type.Integer . round use_bankers=True . value_type . should_equal Value_Type.Float
            col . cast Value_Type.Decimal . round use_bankers=True . value_type . should_equal Value_Type.Float

            col . cast Value_Type.Float . ceil . value_type . should_equal Value_Type.Float
            col . cast Value_Type.Integer . ceil . value_type . should_equal (Value_Type.Decimal 38 0)
            col . cast Value_Type.Decimal . ceil . value_type . should_equal (Value_Type.Decimal 38 0)

            col . cast Value_Type.Float . floor . value_type . should_equal Value_Type.Float
            col . cast Value_Type.Integer . floor . value_type . should_equal (Value_Type.Decimal 38 0)
            col . cast Value_Type.Decimal . floor . value_type . should_equal (Value_Type.Decimal 38 0)

            col . cast Value_Type.Float . truncate . value_type . should_equal Value_Type.Float
            col . cast Value_Type.Integer . truncate . value_type . should_equal (Value_Type.Decimal 38 0)
            col . cast Value_Type.Decimal . truncate . value_type . should_equal (Value_Type.Decimal 38 0)

        do_op n op =
            table = light_table_builder [["x", [n]]]
            result = table.at "x" |> op
            result.to_vector.at 0

        do_round n dp=0 use_bankers=False = do_op n (_.round dp use_bankers)

        group_builder.specify "Can round correctly near the precision limit" <|
            do_round 1.2222222222222225 15 . should_equal 1.222222222222223
            do_round -1.2222222222222225 15 . should_equal -1.222222222222223
            do_round 1.2222222222222235 15 . should_equal 1.222222222222224
            do_round -1.2222222222222235 15 . should_equal -1.222222222222224

        group_builder.specify "Can round correctly near the precision limit, using banker's rounding" <|
            do_round 1.2222222222222225 15 use_bankers=True . should_equal 1.222222222222222
            do_round -1.2222222222222225 15 use_bankers=True . should_equal -1.222222222222222
            do_round 1.2222222222222235 15 use_bankers=True . should_equal 1.222222222222224
            do_round -1.2222222222222235 15 use_bankers=True . should_equal -1.222222222222224

        group_builder.specify "Can handle NaN/Infinity" <|
            nan_result = if setup.test_selection.is_nan_and_nothing_distinct then Number.nan else Nothing
            ops = [.round, .truncate, .ceil, .floor]
            ops.each op->
                do_op Number.nan op . should_equal nan_result
                do_op Number.positive_infinity op . should_equal Number.positive_infinity
                do_op Number.negative_infinity op . should_equal Number.negative_infinity

type Lazy_Ref
   Value ~get

add_snowflake_specs suite_builder create_connection_fn db_name =
    prefix = "[Snowflake] "
    name_counter = Ref.new 0

    ## We prefer to keep a single connection for most tests, to avoid the overhead of initializing a new connection multiple times.
       It is initialized lazily, so that it is actually established only if actually used. Merely listing the tests to run should not establish the connection.
    default_connection = Lazy_Ref.Value (create_connection_fn Nothing)
    table_builder columns connection=Nothing =
        ix = name_counter.get
        name_counter . put ix+1
        name = Name_Generator.random_name "table_"+ix.to_text
        in_mem_table = Table.new columns
        in_mem_table.select_into_database_table (connection.if_nothing default_connection.get) name primary_key=Nothing temporary=True
    light_table_builder columns =
        default_connection.get.base_connection.create_literal_table (Table.new columns) "literal_table"
    materialize = .read

    common_selection = Common_Table_Operations.Main.Test_Selection.Config order_by_unicode_normalization_by_default=True allows_mixed_type_comparisons=False text_length_limited_columns=True fixed_length_text_columns=False different_size_integer_types=False removes_trailing_whitespace_casting_from_char_to_varchar=False supports_decimal_type=True supported_replace_params=supported_replace_params run_advanced_edge_case_tests_by_default=False supports_date_time_without_timezone=True supports_nanoseconds_in_time=True is_nan_comparable=True distinct_returns_first_row_from_group_if_ordered=False
    aggregate_selection = Common_Table_Operations.Aggregate_Spec.Test_Selection.Config first_last_row_order=False aggregation_problems=False text_concat=False
    agg_in_memory_table = ((Project_Description.new enso_dev.Table_Tests).data / "data.csv") . read

    agg_table_fn = _->
        agg_in_memory_table.select_into_database_table default_connection.get (Name_Generator.random_name "Agg1") primary_key=Nothing temporary=True

    empty_agg_table_fn = _->
        (agg_in_memory_table.take (..First 0)).select_into_database_table default_connection.get (Name_Generator.random_name "Agg_Empty") primary_key=Nothing temporary=True

    is_feature_supported_fn feature:Feature = default_connection.get.dialect.is_feature_supported feature
    is_operation_supported_fn operation:Text = default_connection.get.dialect.is_operation_supported operation
    flagged_fn = default_connection.get.dialect.flagged

    setup = Common_Table_Operations.Main.Test_Setup.Config prefix agg_table_fn empty_agg_table_fn table_builder materialize is_database=True test_selection=common_selection aggregate_test_selection=aggregate_selection create_connection_func=create_connection_fn light_table_builder=light_table_builder is_integer_type=is_snowflake_integer is_feature_supported=is_feature_supported_fn flagged=flagged_fn is_operation_supported=is_operation_supported_fn

    Common_Spec.add_specs suite_builder prefix create_connection_fn default_connection setup
    snowflake_specific_spec suite_builder default_connection db_name setup
    Common_Table_Operations.Main.add_specs suite_builder setup
    Upload_Spec.add_specs suite_builder setup create_connection_fn
    IR_Spec.add_specs suite_builder setup prefix default_connection.get

## PRIVATE
is_snowflake_integer value_type = case value_type of
    Value_Type.Integer _ -> True
    Value_Type.Decimal _ scale -> scale == 0
    _ -> False

## PRIVATE
supported_replace_params : Hashset Replace_Params
supported_replace_params =
    e0 = [Replace_Params.Value Text Case_Sensitivity.Default False, Replace_Params.Value Text Case_Sensitivity.Default True, Replace_Params.Value Text Case_Sensitivity.Sensitive False]
    e1 = [Replace_Params.Value Text Case_Sensitivity.Sensitive True, Replace_Params.Value Text Case_Sensitivity.Insensitive False, Replace_Params.Value Text Case_Sensitivity.Insensitive True]
    e2 = [Replace_Params.Value Regex Case_Sensitivity.Default False, Replace_Params.Value Regex Case_Sensitivity.Default True, Replace_Params.Value Regex Case_Sensitivity.Sensitive False]
    e3 = [Replace_Params.Value Regex Case_Sensitivity.Sensitive True, Replace_Params.Value Regex Case_Sensitivity.Insensitive False, Replace_Params.Value Regex Case_Sensitivity.Insensitive True]
    e4 = [Replace_Params.Value DB_Column Case_Sensitivity.Default False, Replace_Params.Value DB_Column Case_Sensitivity.Sensitive False]
    Hashset.from_vector <| e0 + e1 + e2 + e3 + e4

transform_file base_file connection_details =
    content = Data_Link.read_raw_config base_file

    if connection_details.credentials.password.is_a Enso_Secret then
        Panic.throw (Illegal_State.Error "Currently Snowflake tests need a plaintext password in environment variable to run.")

    new_content = content
        . replace "ACCOUNT" connection_details.account
        . replace "DBNAME" connection_details.database
        . replace "SCHEMA" connection_details.schema
        . replace "WAREHOUSE" connection_details.warehouse
        . replace "USERNAME" connection_details.credentials.username
        . replace "PASSWORD" connection_details.credentials.password

    temp_file = File.create_temporary_file "snowflake-test-db" ".datalink"
    Data_Link.write_raw_config temp_file new_content replace_existing=True . if_not_error temp_file

type Temporary_Data_Link_File
    Value ~get

    make connection_details = Temporary_Data_Link_File.Value <|
        transform_file (enso_project.data / "datalinks" / "snowflake-db.datalink") connection_details

add_table_specs suite_builder =
    case get_configured_connection_details of
        Nothing ->
            message = "Snowflake test connection is not configured. See README.md for instructions."
            suite_builder.group "[Snowflake] Database tests" pending=message (_-> Nothing)
        connection_details ->
            db_name = connection_details.database
            connection_builder = _ -> Database.connect connection_details
            add_snowflake_specs suite_builder connection_builder db_name
            prefix = "[Snowflake] "
            Transaction_Spec.add_specs suite_builder connection_builder prefix

            data_link_file = Temporary_Data_Link_File.make connection_details
            Audit_Spec.add_specs suite_builder prefix data_link_file.get database_pending=Nothing
            Save_Connection_Data_Link.add_specs suite_builder prefix connection_details pending=Nothing

            suite_builder.group "[Snowflake] Secrets in connection settings" group_builder->
                cloud_setup = Cloud_Tests_Setup.prepare
                base_details = get_configured_connection_details
                group_builder.specify "should allow to set up a connection with the password passed as a secret" pending=cloud_setup.pending <|
                    cloud_setup.with_prepared_environment <|
                        with_secret "my_snowflake_username" base_details.credentials.username username_secret-> with_secret "my_snowflake_password" base_details.credentials.password password_secret->
                            secret_credentials = Credentials.Username_And_Password username_secret password_secret
                            details = Snowflake_Details.Snowflake base_details.account secret_credentials base_details.database base_details.schema base_details.warehouse
                            connection = Database.connect details
                            connection.should_succeed
                            Panic.with_finalizer connection.close <|
                                connection.tables . should_be_a Table

with_secret name value callback = case value of
    # If it is already a secret, we pass it as-is.
    _ : Enso_Secret -> callback value
    # Otherwise we create the secret, and clean it up afterwards.
    _ : Text ->
        secret = Enso_Secret.create name+Random.uuid value
        secret.should_succeed
        Panic.with_finalizer secret.delete (callback secret)

## We rethrow any dataflow errors to ensure that they are reported.
   Without it, a dataflow error could make some tests just not be registered and
   not run, without displaying any failures.
get_configured_connection_details = Panic.rethrow <|
    account_name = Environment.get "ENSO_SNOWFLAKE_ACCOUNT"
    if account_name.is_nothing then Nothing else
        if account_name.is_empty then
            Panic.throw (Illegal_Argument.Error "ENSO_SNOWFLAKE_ACCOUNT is set, but empty. Please set all required environment variables.")

        get_var name =
            value = Environment.get name if_missing=(Panic.throw (Illegal_State.Error "ENSO_SNOWFLAKE_ACCOUNT is set, but "+name+" is not. Please set all required environment variables."))
            if value.is_empty then
                Panic.throw (Illegal_State.Error "The "+name+" environment variable is set, but it is empty. Please set all required environment variables.")
            value
        user = get_var "ENSO_SNOWFLAKE_USER"
        password = get_var "ENSO_SNOWFLAKE_PASSWORD"
        database = get_var "ENSO_SNOWFLAKE_DATABASE"
        schema = Environment.get "ENSO_SNOWFLAKE_SCHEMA" if_missing="PUBLIC"
        warehouse = Environment.get "ENSO_SNOWFLAKE_WAREHOUSE" if_missing=""

        resolved_password = if password.starts_with "enso://" then Enso_Secret.get password else password
        credentials = Credentials.Username_And_Password user resolved_password
        Snowflake_Details.Snowflake account_name credentials database schema warehouse

add_specs suite_builder =
    add_table_specs suite_builder

main filter=Nothing =
    suite = Test.build suite_builder->
        add_specs suite_builder
    suite.run_with_filter filter
