from Standard.Base import all
import Standard.Base.Runtime.Ref.Ref
from Standard.Base.Runtime import assert
import Standard.Base.Errors.File_Error.File_Error
import Standard.Base.Errors.Illegal_Argument.Illegal_Argument

from Standard.Table import Table, Value_Type, Bits
from Standard.Table.Errors import Invalid_Column_Names, Duplicate_Output_Column_Names

import Standard.Database.Internal.Replace_Params.Replace_Params
import Standard.Database.Feature.Feature
import Standard.Database.Dialect_Flag.Dialect_Flag
from Standard.Database import all
from Standard.Database.Errors import SQL_Error, Unsupported_Database_Operation, Unsupported_Database_Type

from Standard.Test import all

import project.Database.Common.Common_Spec
import project.Database.Common.IR_Spec
import project.Database.Transaction_Spec
import project.Database.Upload_Spec
import project.Database.Types.SQLite_Type_Mapping_Spec
import project.Database.Helpers.Name_Generator
import project.Common_Table_Operations

type Test_Data
    Value ~connection

    setup create_connection_func =
        connection = create_connection_func Nothing
        Test_Data.Value connection

    teardown self =
        self.connection.close


type Metadata_Data
    Value ~data

    connection self = self.data.at 0
    tinfo self = self.data.at 1
    t self = self.data.at 2

    setup create_connection_func = Metadata_Data.Value <|
        connection = create_connection_func Nothing
        tinfo = Name_Generator.random_name "Tinfo"
        connection.execute_update 'CREATE TABLE "'+tinfo+'" ("strs" VARCHAR, "ints" INTEGER, "bools" BOOLEAN, "reals" REAL)'
        t = connection.query (SQL_Query.Table_Name tinfo)
        row1 = ["a", Nothing, False, 1.2]
        row2 = ["abc", Nothing, Nothing, 1.3]
        row3 = ["def", 42, True, 1.4]
        Panic.rethrow <|
            t.update_rows (Table.from_rows ["strs", "ints", "bools", "reals"] [row1, row2, row3]) update_action=Update_Action.Insert
        [connection, tinfo, t]

    teardown self =
        self.connection.drop_table self.t.name
        self.connection.drop_table self.tinfo
        self.connection.close


type Tables_And_Table_Types_Data
    Value ~data

    connection self = self.data.at 0
    tinfo self = self.data.at 1
    vinfo self = self.data.at 2
    temporary_table self = self.data.at 3

    setup create_connection_func = Tables_And_Table_Types_Data.Value <|
        connection = create_connection_func Nothing
        tinfo = Name_Generator.random_name "TestTable"
        connection.execute_update 'CREATE TABLE "'+tinfo+'" ("A" VARCHAR)'

        vinfo = Name_Generator.random_name "TestView"
        connection.execute_update 'CREATE VIEW "'+vinfo+'" AS SELECT "A" FROM "'+tinfo+'";'

        temporary_table = Name_Generator.random_name "TemporaryTable"
        (Table.new [["X", [1, 2, 3]]]).select_into_database_table connection temporary_table temporary=True

        [connection, tinfo, vinfo, temporary_table]

    teardown self =
        self.connection.drop_table self.tinfo
        self.connection.drop_table self.vinfo
        self.connection.drop_table self.temporary_table
        self.connection.close


sqlite_specific_spec suite_builder prefix create_connection_func setup =
    table_builder = setup.table_builder

    suite_builder.group prefix+"Schemas and Databases" group_builder->
        data = Test_Data.setup create_connection_func

        group_builder.teardown <|
            data.teardown

        group_builder.specify "should be able to get current database and list databases" <|
            data.connection.database . should_equal Nothing
            data.connection.databases . should_equal [Nothing]
            Meta.is_same_object data.connection (data.connection.set_database Nothing) . should_be_true

        group_builder.specify "should be able to get current schema and list schemas" <|
            data.connection.schema . should_equal Nothing
            data.connection.schemas . should_equal [Nothing]
            Meta.is_same_object data.connection (data.connection.set_schema Nothing) . should_be_true

        group_builder.specify "does not allow changing schema or database" <|
            data.connection.set_schema "foo" . should_fail_with SQL_Error
            data.connection.set_database "foo" . should_fail_with SQL_Error

    suite_builder.group prefix+"Tables and Table Types" group_builder->
        data = Tables_And_Table_Types_Data.setup create_connection_func

        group_builder.teardown <|
            data.teardown

        group_builder.specify "should be able to list table types" <|
            table_types = data.connection.table_types
            table_types.length . should_not_equal 0
            table_types.contains "TABLE" . should_be_true
            table_types.contains "VIEW" . should_be_true

        group_builder.specify "should be able to list tables" <|
            tables = data.connection.tables
            tables.row_count . should_not_equal 0
            tables.columns.map .name . should_equal ["Database", "Schema", "Name", "Type", "Description"]

            table_names = tables.at "Name" . to_vector
            table_names.should_contain data.tinfo
            table_names.should_contain data.vinfo
            table_names.should_contain data.temporary_table

        group_builder.specify "should be able to filter tables by name" <|
            tables = data.connection.tables data.tinfo
            tables.row_count . should_equal 1
            tables.at "Database" . to_vector . at 0 . should_equal Nothing
            tables.at "Schema" . to_vector . at 0 . should_equal Nothing
            tables.at "Name" . to_vector . at 0 . should_equal data.tinfo
            tables.at "Type" . to_vector . at 0 . should_equal "TABLE"

            data.connection.tables "TestT_ble%" . row_count . should_equal 1
            data.connection.tables "Temporary%ble%" . row_count . should_equal 1
            data.connection.tables "Temporary%ble%" . at "Type" . to_vector . should_equal ["GLOBAL TEMPORARY"]
            data.connection.tables "N_nexistent%" . row_count . should_equal 0

        group_builder.specify "should be able to filter tables by type" <|
            tables = data.connection.tables types=["VIEW"]
            tables.row_count . should_not_equal 0
            tables.at "Name" . to_vector . contains data.tinfo . should_be_false
            tables.at "Name" . to_vector . contains data.vinfo . should_be_true

    suite_builder.group prefix+"Error Handling" group_builder->
        data = Test_Data.setup create_connection_func

        group_builder.teardown <|
            data.teardown

        group_builder.specify "should wrap errors" <|
            data.connection.read (SQL_Query.Raw_SQL "foobar") . should_fail_with SQL_Error
            data.connection.execute_update "foobar" . should_fail_with SQL_Error

            action = data.connection.read (SQL_Query.Raw_SQL "SELECT A FROM undefined_table")
            action . should_fail_with SQL_Error
            action.catch.to_text . should_equal "There was an SQL error: [SQLITE_ERROR] SQL error or missing database (no such table: undefined_table). [Query was: SELECT A FROM undefined_table]"

        group_builder.specify "is capable of handling weird tables" <|
            Problems.assume_no_problems <|
                data.connection.execute_update 'CREATE TEMPORARY TABLE "empty-column-name" ("" VARCHAR)'
            t1 = data.connection.query (SQL_Query.Table_Name "empty-column-name")
            Problems.expect_only_warning Invalid_Column_Names t1
            t1.column_names . should_equal ["Column 1"]
            m1 = t1.read
            m1.at "Column 1" . to_vector . should_equal []

            Problems.assume_no_problems <|
                data.connection.execute_update 'CREATE TEMPORARY TABLE "clashing-unicode-names" ("ś" VARCHAR, "s\u0301" INTEGER)'
            Problems.assume_no_problems <|
                data.connection.execute_update 'INSERT INTO "clashing-unicode-names" VALUES (\'A\', 2)'
            t2 = data.connection.query (SQL_Query.Table_Name "clashing-unicode-names")
            Problems.expect_only_warning Duplicate_Output_Column_Names t2
            t2.column_names . should_equal ["ś", "ś 1"]
            m2 = t2.read
            m2.at "ś"   . to_vector . should_equal ["A"]
            m2.at "ś 1" . to_vector . should_equal [2]

            r3 = data.connection.query (..Raw_SQL 'SELECT 1 AS "A", 2 AS "A"')
            r3.should_fail_with Illegal_Argument
            r3.catch.cause . should_be_a Duplicate_Output_Column_Names

            r4 = data.connection.query (..Raw_SQL 'SELECT 1 AS ""')
            r4.should_fail_with Illegal_Argument
            r4.catch.cause . should_be_a Invalid_Column_Names

    suite_builder.group prefix+"Metadata" group_builder->
        data = Metadata_Data.setup create_connection_func

        group_builder.teardown <|
            data.teardown

        group_builder.specify "should return Table information" <|
            i = data.t.column_info
            i.at "Column" . to_vector . should_equal ["strs", "ints", "bools", "reals"]
            i.at "Items Count" . to_vector . should_equal [3, 1, 2, 3]
            i.at "Value Type" . to_vector . should_equal [Value_Type.Char, Value_Type.Integer, Value_Type.Boolean, Value_Type.Float]
        group_builder.specify "should infer standard types correctly" <|
            data.t.at "strs" . value_type . is_text . should_be_true
            data.t.at "ints" . value_type . is_integer . should_be_true
            data.t.at "bools" . value_type . is_boolean . should_be_true
            data.t.at "reals" . value_type . is_floating_point . should_be_true

            data.t.at "ints" . value_type . is_text . should_be_false
            data.t.at "strs" . value_type . is_integer . should_be_false
            data.t.at "reals" . value_type . is_boolean . should_be_false
            data.t.at "bools" . value_type . is_floating_point . should_be_false

    suite_builder.group prefix+"Dialect-specific codegen" group_builder->
        data = Metadata_Data.setup create_connection_func

        group_builder.teardown <|
            data.teardown

        group_builder.specify "should generate queries for the Distinct operation" <|
            t = data.connection.query (SQL_Query.Table_Name data.tinfo)
            code_template = 'SELECT "{Tinfo}"."strs" AS "strs", "{Tinfo}"."ints" AS "ints", "{Tinfo}"."bools" AS "bools", "{Tinfo}"."reals" AS "reals" FROM (SELECT "{Tinfo}_inner"."strs" AS "strs", "{Tinfo}_inner"."ints" AS "ints", "{Tinfo}_inner"."bools" AS "bools", "{Tinfo}_inner"."reals" AS "reals" FROM (SELECT "{Tinfo}"."strs" AS "strs", "{Tinfo}"."ints" AS "ints", "{Tinfo}"."bools" AS "bools", "{Tinfo}"."reals" AS "reals" FROM "{Tinfo}" AS "{Tinfo}") AS "{Tinfo}_inner" GROUP BY "{Tinfo}_inner"."strs") AS "{Tinfo}"'
            expected_code = code_template.replace "{Tinfo}" data.tinfo
            t.distinct ["strs"] . to_sql . prepare . should_equal [expected_code, []]

    suite_builder.group prefix+"math functions" group_builder->
        data = Test_Data.setup create_connection_func

        group_builder.teardown <|
            data.teardown

        group_builder.specify "round, trunc, ceil, floor" <|
            col = (table_builder [["x", [0.1, 0.9, 3.1, 3.9, -0.1, -0.9, -3.1, -3.9]]] connection=data.connection) . at "x"

            col . cast Value_Type.Float . round . value_type . should_equal Value_Type.Float
            col . cast Value_Type.Integer . round . value_type . should_equal Value_Type.Float
            col . cast Value_Type.Decimal . round . value_type . should_equal Value_Type.Float

            col . cast Value_Type.Float . round 1 . value_type . should_equal Value_Type.Float
            col . cast Value_Type.Integer . round 1 . value_type . should_equal Value_Type.Float
            col . cast Value_Type.Decimal . round 1 . value_type . should_equal Value_Type.Float

            col . cast Value_Type.Float . round use_bankers=True . value_type . should_equal Value_Type.Float
            col . cast Value_Type.Integer . round use_bankers=True . value_type . should_equal Value_Type.Float
            col . cast Value_Type.Decimal . round use_bankers=True . value_type . should_equal Value_Type.Float

            col . cast Value_Type.Float . ceil . value_type . should_equal Value_Type.Float
            col . cast Value_Type.Integer . ceil . value_type . should_equal Value_Type.Integer
            col . cast Value_Type.Decimal . ceil . value_type . should_equal Value_Type.Float

            col . cast Value_Type.Float . floor . value_type . should_equal Value_Type.Float
            col . cast Value_Type.Integer . floor . value_type . should_equal Value_Type.Integer
            col . cast Value_Type.Decimal . floor . value_type . should_equal Value_Type.Float

            col . cast Value_Type.Float . truncate . value_type . should_equal Value_Type.Float
            col . cast Value_Type.Integer . truncate . value_type . should_equal Value_Type.Integer
            col . cast Value_Type.Decimal . truncate . value_type . should_equal Value_Type.Float

        do_op data n op =
            table = table_builder [["x", [n]]] connection=data.connection
            result = table.at "x" |> op
            result.to_vector.at 0

        do_round data n dp=0 use_bankers=False = do_op data n (_.round dp use_bankers)

        group_builder.specify "Can round correctly near the precision limit" <|
            # This value varies depending on the version of SQLite.
            do_round data 1.2222222222222225 15 . should_equal 1.222222222222223 0.000000000000002
            do_round data -1.2222222222222225 15 . should_equal -1.222222222222223 0.000000000000002
            do_round data 1.2222222222222235 15 . should_equal 1.222222222222223
            do_round data -1.2222222222222235 15 . should_equal -1.222222222222223

        group_builder.specify "Can round correctly near the precision limit, using banker's rounding" <|
            do_round data 1.2222222222222225 15 use_bankers=True . should_equal 1.222222222222222
            do_round data -1.2222222222222225 15 use_bankers=True . should_equal -1.222222222222222
            do_round data 1.2222222222222235 15 use_bankers=True . should_equal 1.222222222222224
            do_round data -1.2222222222222235 15 use_bankers=True . should_equal -1.222222222222224

        group_builder.specify "Can handle NaN/Infinity" <|
            nan_result = if setup.test_selection.is_nan_and_nothing_distinct then Number.nan else Nothing
            ops = [.round, .truncate, .ceil, .floor]
            ops.each op->
                do_op data Number.nan op . should_equal nan_result
                do_op data Number.positive_infinity op . should_equal Number.positive_infinity
                do_op data Number.negative_infinity op . should_equal Number.negative_infinity

        group_builder.specify "round returns the correct type" <|
            do_round data 231.2 1 . should_be_a Float
            do_round data 231.2 0 . should_be_a Float
            do_round data 231.2 . should_be_a Float
            do_round data 231.2 -1 . should_be_a Float

        group_builder.specify "round returns the correct type" <|
            do_round data 231 1 . should_be_a Float
            do_round data 231 0 . should_be_a Float
            do_round data 231 . should_be_a Float
            do_round data 231 -1 . should_be_a Float

    suite_builder.group prefix+"Column.const" group_builder->
        data = Test_Data.setup create_connection_func

        group_builder.teardown <|
            data.teardown

        group_builder.specify "Does not support making a constant column from a Date" <|
            t = table_builder [["x", ["1", "2", "3"]]] connection=data.connection
            t.at "x" . const (Date.new 12 4 12) . should_fail_with Unsupported_Database_Type

type Lazy_Ref
   Value ~get

sqlite_spec suite_builder prefix create_connection_func persistent_connector =
    name_counter = Ref.new 0
    # We keep a default connection to avoid creating connections each time.
    default_connection = Lazy_Ref.Value (create_connection_func Nothing)
    # The default `connection` parameter always create a new connection.
    # In some tests, for example, where we are joining tables, we have to specify
    # exactly the same connection.
    table_builder columns connection=Nothing =
        ix = name_counter.get
        name_counter . put ix+1
        name = Name_Generator.random_name "table_"+ix.to_text

        in_mem_table = Table.new columns
        in_mem_table.select_into_database_table (connection.if_nothing default_connection.get) name primary_key=Nothing
    light_table_builder columns =
        default_connection.get.base_connection.create_literal_table (Table.new columns) "literal_table"

    materialize = .read

    common_selection = Common_Table_Operations.Main.Test_Selection.Config natural_ordering=False case_insensitive_ordering=True case_insensitive_ascii_only=True is_nan_and_nothing_distinct=False date_time=False supported_replace_params=supported_replace_params different_size_integer_types=False length_restricted_text_columns=False char_max_size_after_substring=..Reset run_advanced_edge_case_tests_by_default=True

    ## For now `advanced_stats`, `text_shortest_longest` and
       `multi_distinct` remain disabled, because SQLite does not provide the
       needed aggregate functions and emulating them is highly problematic.
       We can rethink in the future how these could be emulated. Two of the
       possible solutions are:
       - creating complex nested queries using NTILE to compute the stats,
       - compiling SQLite library on our own and adding native extensions for
         the missing statistics.
    aggregate_selection = Common_Table_Operations.Aggregate_Spec.Test_Selection.Config advanced_stats=False text_shortest_longest=False first_last_row_order=False first_last_ignore_nothing=False multi_distinct=False aggregation_problems=False nan=False date_support=False
    agg_in_memory_table = (enso_project.data / "data.csv") . read

    agg_table_fn = _ ->
        agg_in_memory_table.select_into_database_table default_connection.get (Name_Generator.random_name "Agg1") primary_key=Nothing temporary=True

    empty_agg_table_fn = _ ->
        (agg_in_memory_table.take (..First 0)).select_into_database_table default_connection.get (Name_Generator.random_name "Agg_Empty") primary_key=Nothing temporary=True

    is_feature_supported_fn feature:Feature = default_connection.get.dialect.is_feature_supported feature
    is_operation_supported_fn operation:Text = default_connection.get.dialect.is_operation_supported operation
    flagged_fn = default_connection.get.dialect.flagged

    setup = Common_Table_Operations.Main.Test_Setup.Config prefix agg_table_fn empty_agg_table_fn table_builder materialize is_database=True test_selection=common_selection aggregate_test_selection=aggregate_selection create_connection_func=create_connection_func light_table_builder=light_table_builder is_feature_supported=is_feature_supported_fn flagged=flagged_fn is_operation_supported=is_operation_supported_fn
    
    Common_Spec.add_specs suite_builder prefix create_connection_func default_connection setup
    sqlite_specific_spec suite_builder prefix create_connection_func setup
    Common_Table_Operations.Main.add_specs suite_builder setup
    Upload_Spec.add_specs suite_builder setup create_connection_func persistent_connector=persistent_connector
    IR_Spec.add_specs suite_builder setup prefix default_connection.get


## PRIVATE
supported_replace_params : Hashset Replace_Params
supported_replace_params =
    e = [Replace_Params.Value Text Case_Sensitivity.Default False, Replace_Params.Value Text Case_Sensitivity.Sensitive False, Replace_Params.Value Text Case_Sensitivity.Default True, Replace_Params.Value Text Case_Sensitivity.Sensitive True, Replace_Params.Value Text Case_Sensitivity.Insensitive True]
    Hashset.from_vector e

## Reference to the database file that ensures the first test that uses it will
   clean any leftover files from earlier runs.
type Database_File
    Value ~file

    create = Database_File.Value <|
        transient_dir = enso_project.data / "transient"
        assert transient_dir.exists ("The directory " + transient_dir.path + " should exist (ensured by containing a `.gitignore` file).")
        f = transient_dir / "sqlite_test.db"
        f.delete_if_exists
        f

create_inmem_connection =
    Database.connect (SQLite.In_Memory)

create_file_connection file =
    connection = Database.connect (SQLite.From_File file)
    connection.execute_update 'CREATE TABLE "Dummy" ("strs" VARCHAR, "ints" INTEGER, "bools" BOOLEAN, "reals" REAL)'
    connection

type File_Connection
    Value ~file

    setup db_file:Database_File = File_Connection.Value <|
        tmp_file = db_file.file
        con = create_file_connection tmp_file
        con.close
        assert tmp_file.exists
        tmp_file

    teardown self =
        assert self.file.exists
        self.file.delete


add_specs suite_builder =
    in_file_prefix = "[SQLite File] "
    # TODO: Add a suite-level teardown to delete this file at the end.
    # https://github.com/enso-org/enso/issues/9436
    # https://github.com/enso-org/enso/issues/9437
    database_file = Database_File.create

    sqlite_spec suite_builder in_file_prefix (_ -> create_file_connection database_file.file) persistent_connector=True
    Transaction_Spec.add_specs suite_builder (_ -> create_file_connection database_file.file) in_file_prefix

    in_memory_prefix = "[SQLite In-Memory] "
    sqlite_spec suite_builder in_memory_prefix (_ -> create_inmem_connection) persistent_connector=False
    Transaction_Spec.add_specs suite_builder (_ -> create_inmem_connection) in_memory_prefix

    SQLite_Type_Mapping_Spec.add_specs suite_builder

    suite_builder.group "SQLite_Format should allow connecting to SQLite files" group_builder->
        data = File_Connection.setup database_file

        group_builder.specify "should recognise a SQLite database file" <|
            Auto_Detect.get_reading_format data.file . should_be_a SQLite_Format

        group_builder.specify "should recognise a sqlite file by extension for writing" <|
            Auto_Detect.get_writing_format (enso_project.data / "nonexistent-data.db") . should_be_a SQLite_Format
            Auto_Detect.get_writing_format (enso_project.data / "nonexistent-data.sqlite") . should_be_a SQLite_Format

        group_builder.specify "should not recognise nonexistent or empty files for reading" <|
            r1 = Data.read (enso_project.data / "nonexistent-data.db")
            r1.should_fail_with File_Error
            r1.catch . should_be_a File_Error.Not_Found

            empty = enso_project.data / "transient" / "empty-data.db"
            "".write empty on_existing_file=Existing_File_Behavior.Overwrite . should_succeed
            r2 = Data.read empty
            r2.should_fail_with File_Error
            r2.catch . should_be_a File_Error.Unsupported_Type
            empty.delete_if_exists

            broken = enso_project.data / "transient" / "empty-data.db"
            "SOME_RANDOM_DATA".write empty on_existing_file=Existing_File_Behavior.Overwrite . should_succeed
            r3 = Data.read broken
            r3.should_fail_with File_Error
            r3.catch . should_be_a File_Error.Unsupported_Type
            broken.delete_if_exists

        group_builder.specify "should connect to a db file" <|
            connection = Data.read data.file
            tables = connection.tables
            tables.row_count . should_be_a Integer
            connection.close

        group_builder.specify 'should not duplicate warnings' <|
            c = Database.connect (SQLite.In_Memory)
            t0 = Table.new [["X", ["a", "bc", "def"]]]
            t1 = t0.select_into_database_table c "Tabela"
            t2 = t1.cast "X" (Value_Type.Char size=1)
            Warning.get_all t2 . length . should_equal 1

main filter=Nothing =
    suite = Test.build suite_builder->
        add_specs suite_builder
    suite.run_with_filter filter
