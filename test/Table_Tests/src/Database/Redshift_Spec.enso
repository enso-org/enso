from Standard.Base import all
import Standard.Base.Runtime.Ref

from Standard.Table import Table

from Standard.Database import Database, Redshift, AWS_Credential, SQL_Query

from Standard.Test import Test, Test_Suite

import project.Database.Common_Spec
import project.Database.Helpers.Name_Generator
import project.Common_Table_Spec
import project.Aggregate_Spec

redshift_specific_spec connection pending =
    Test.group "[Redshift] Info" pending=pending <|
        tinfo = Name_Generator.random_name "Tinfo"
        connection.execute_update 'CREATE TEMPORARY TABLE "'+tinfo+'" ("strs" VARCHAR, "ints" INTEGER, "bools" BOOLEAN, "reals" REAL)'
        t = connection.query (SQL_Query.Table_Name tinfo)
        t.insert ["a", Nothing, False, 1.2]
        t.insert ["abc", Nothing, Nothing, 1.3]
        t.insert ["def", 42, True, 1.4]
        Test.specify "should return Table information" <|
            i = t.info
            i.index . to_vector . should_equal ["strs", "ints", "bools", "reals"]
            i.at "Items Count" . to_vector . should_equal [3, 1, 2, 3]
            i.at "SQL Type" . to_vector . should_equal ["varchar", "int4", "bool", "float4"]
        Test.specify "should infer standard types correctly" <|
            t.at "strs" . sql_type . is_definitely_text . should_be_true
            t.at "ints" . sql_type . is_definitely_integer . should_be_true
            t.at "bools" . sql_type . is_definitely_boolean . should_be_true
            t.at "reals" . sql_type . is_definitely_double . should_be_true
        connection.execute_update 'DROP TABLE "'+tinfo+'"'

run_tests connection pending=Nothing =
    prefix = "[Redshift] "
    name_counter = Ref.new 0
    tables = Vector.new_builder
    table_builder columns =
        ix = name_counter.get
        name_counter . put ix+1
        name = Name_Generator.random_name "table_"+ix.to_text

        in_mem_table = Table.new columns
        case connection.upload_table name in_mem_table of
            table ->
                tables.append name
                table
    clean_tables table_names =
        table_names.each name->
            sql = 'DROP TABLE "' + name + '"'
            Panic.rethrow <| connection.execute_update sql
    materialize = .read

    Common_Spec.spec prefix connection pending=pending
    redshift_specific_spec connection pending=pending
    common_selection = Common_Table_Spec.Test_Selection.Config supports_case_sensitive_columns=False order_by=False take_drop=False
    Common_Table_Spec.spec prefix table_builder test_selection=common_selection pending=pending

    selection = Aggregate_Spec.Test_Selection.Config text_concat=False text_shortest_longest=False first_last=False first_last_row_order=False multi_distinct=False aggregation_problems=False date_support=False
    agg_in_memory_table = (enso_project.data / "data.csv") . read
    agg_table = connection.upload_table (Name_Generator.random_name "Agg1") agg_in_memory_table
    tables.append agg_table.name
    empty_agg_table = connection.upload_table (Name_Generator.random_name "Agg_Empty") (agg_in_memory_table.take (First 0))
    tables.append empty_agg_table.name
    Aggregate_Spec.aggregate_spec prefix agg_table empty_agg_table table_builder materialize is_database=True selection pending=pending

    clean_tables tables.to_vector

connect_via_json_config =
    credentials = enso_project.data / 'redshift_credentials.json'
    msg = "Redshift connection is not set up. Please create a JSON file containing the credentials in `data/redshift_credentials.json`"

    if credentials.exists.not then msg else
        creds = Json.parse credentials.read_text . unwrap
        access_key = creds.get 'access_key_id'
        secret_key = creds.get 'secret_access_key'
        uri = uri_parse (creds.get 'db_uri')
        db_uri = uri.at 0
        db_port = uri.at 1
        db_name = uri.at 2

        user = creds.get 'db_user'
        Redshift db_uri db_port db_name credentials=(AWS_Credential.Key user access_key secret_key)

connect_via_aws_environment db_host_port =
    db_host_port_split = uri_parse db_host_port
    db_uri = db_host_port_split.at 0
    db_port = db_host_port_split.at 1
    db_name = db_host_port_split.at 2

    db_user = Environment.get "ENSO_REDSHIFT_USER"
    access_key = Environment.get "AWS_ACCESS_KEY_ID"
    secret_key = Environment.get "AWS_SECRET_ACCESS_KEY"

    credentials = if (access_key.is_nothing || secret_key.is_nothing) then AWS_Credential.Profile db_user (Environment.get "AWS_PROFILE" . if_nothing '') else
        AWS_Credential.Key db_user access_key secret_key

    Redshift db_uri db_port db_name credentials=credentials

uri_parse uri =
    host_db_split = uri.split '/'
    host_split = host_db_split.at 0 . split ':'

    db_host = host_split.first
    db_port = if host_split.length == 1 then 5439 else
        Integer.parse (host_split.at 1)

    db_name = if host_db_split.length == 1 then '' else host_db_split.at 1
    [db_host, db_port, db_name]

spec =
    db_host_port = Environment.get "ENSO_REDSHIFT_URI"
    connection_details = if db_host_port.is_nothing then connect_via_json_config else
        connect_via_aws_environment db_host_port

    case connection_details of
        _ : Text ->
            connection = Error.throw connection_details
            run_tests connection pending=connection_details
        _ ->
            connection = Database.connect connection_details
            run_tests connection

main = Test_Suite.run_main spec
