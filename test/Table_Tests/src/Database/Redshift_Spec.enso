from Standard.Base import all
import Standard.Base.Runtime.Ref.Ref

import Standard.Table.Data.Type.Value_Type.Bits
from Standard.Table import Table, Value_Type

from Standard.Database import all

from Standard.AWS import Redshift_Details, AWS_Credential

from Standard.Test import Test, Test_Suite
import Standard.Test.Extensions

import project.Database.Common.Common_Spec
import project.Database.Helpers.Name_Generator
import project.Common_Table_Operations

redshift_specific_spec connection =
    Test.group "[Redshift] Info" <|
        tinfo = Name_Generator.random_name "Tinfo"
        connection.execute_update 'CREATE TEMPORARY TABLE "'+tinfo+'" ("strs" VARCHAR, "ints" INTEGER, "bools" BOOLEAN, "reals" REAL)'
        t = connection.query (SQL_Query.Table_Name tinfo)
        row1 = ["a", Nothing, False, 1.2]
        row2 = ["abc", Nothing, Nothing, 1.3]
        row3 = ["def", 42, True, 1.4]
        Panic.rethrow <|
            (Table.from_rows ["strs", "ints", "bools", "reals"] [row1, row2, row3]).update_database_table t update_action=Update_Action.Insert

        Test.specify "should return Table information" <|
            i = t.info
            i.at "Column" . to_vector . should_equal ["strs", "ints", "bools", "reals"]
            i.at "Items Count" . to_vector . should_equal [3, 1, 2, 3]
            i.at "Value Type" . to_vector . should_equal [Value_Type.Char, Value_Type.Integer Bits.Bits_32, Value_Type.Boolean, Value_Type.Float Bits.Bits_32]
        Test.specify "should infer standard types correctly" <|
            t.at "strs" . value_type . is_text . should_be_true
            t.at "ints" . value_type . is_integer . should_be_true
            t.at "bools" . value_type . is_boolean . should_be_true
            t.at "reals" . value_type . is_floating_point . should_be_true
        connection.execute_update 'DROP TABLE "'+tinfo+'"'

run_tests connection =
    prefix = "[Redshift] "
    name_counter = Ref.new 0
    tables = Vector.new_builder
    table_builder columns =
        ix = name_counter.get
        name_counter . put ix+1
        name = Name_Generator.random_name "table_"+ix.to_text

        in_mem_table = Table.new columns
        in_mem_table.select_into_database_table connection name primary_key=Nothing temporary=True
    materialize = .read

    Common_Spec.spec prefix connection
    redshift_specific_spec connection

    common_selection = Common_Table_Operations.Main.Test_Selection.Config supports_case_sensitive_columns=True order_by_unicode_normalization_by_default=True take_drop=False allows_mixed_type_comparisons=False supports_decimal_type=True
    aggregate_selection = Common_Table_Operations.Aggregate_Spec.Test_Selection.Config first_last_row_order=False aggregation_problems=False date_support=False
    agg_in_memory_table = (enso_project.data / "data.csv") . read
    agg_table = agg_in_memory_table.select_into_database_table connection (Name_Generator.random_name "Agg1") primary_key=Nothing temporary=True
    tables.append agg_table.name
    empty_agg_table = (agg_in_memory_table.take (First 0)).select_into_database_table connection (Name_Generator.random_name "Agg_Empty") primary_key=Nothing temporary=True
    tables.append empty_agg_table.name

    setup = Common_Table_Operations.Main.Test_Setup.Config prefix agg_table empty_agg_table table_builder materialize is_database=True test_selection=common_selection aggregate_test_selection=aggregate_selection connection=connection
    Common_Table_Operations.Main.spec setup

connect_via_json_config =
    credentials = enso_project.data / 'redshift_credentials.json'
    msg = "Redshift connection is not set up. Please create a JSON file containing the credentials in `data/redshift_credentials.json`"

    if credentials.exists.not then msg else
        creds = Json.parse credentials.read_text
        access_key_id = creds.get 'access_key_id'
        secret_key = creds.get 'secret_access_key'
        uri = uri_parse (creds.get 'db_uri')
        db_uri = uri.at 0
        db_port = uri.at 1
        db_name = uri.at 2

        user = creds.get 'db_user'
        Redshift_Details.Redshift db_uri db_port db_name user credentials=(AWS_Credential.Key access_key_id secret_key)

connect_via_aws_environment db_host_port =
    db_host_port_split = uri_parse db_host_port
    db_uri = db_host_port_split.at 0
    db_port = db_host_port_split.at 1
    db_name = db_host_port_split.at 2

    db_user = Environment.get "ENSO_REDSHIFT_USER"
    access_key_id = Environment.get "AWS_ACCESS_KEY_ID"
    secret_key = Environment.get "AWS_SECRET_ACCESS_KEY"

    credentials = if (access_key_id.is_nothing || secret_key.is_nothing) then AWS_Credential.Profile (Environment.get "AWS_PROFILE" . if_nothing '') else
        AWS_Credential.Key access_key_id secret_key

    Redshift_Details.Redshift db_uri db_port db_name db_user credentials=credentials

uri_parse uri =
    host_db_split = uri.split '/'
    host_split = host_db_split.at 0 . split ':'

    db_host = host_split.first
    db_port = if host_split.length == 1 then 5439 else
        Integer.parse (host_split.at 1)

    db_name = if host_db_split.length == 1 then '' else host_db_split.at 1
    [db_host, db_port, db_name]

spec =
    db_host_port = Environment.get "ENSO_REDSHIFT_URI"
    connection_details = if db_host_port.is_nothing then connect_via_json_config else
        connect_via_aws_environment db_host_port

    case connection_details of
        _ : Text ->
            Test.group "[Redshift] Database tests" pending=connection_details Nothing
        _ ->
            connection = Database.connect connection_details
            run_tests connection

main = Test_Suite.run_main spec
