from Standard.Base import all
import Standard.Base.Enso_Cloud.Data_Link.Data_Link
import Standard.Base.Enso_Cloud.Internal.Audit_Log.Audit_Log

from Standard.Table import Table

from Standard.Database import all

from Standard.Test import all

import enso_dev.Base_Tests.Network.Enso_Cloud.Cloud_Tests_Setup.Cloud_Tests_Setup
import enso_dev.Base_Tests.Network.Enso_Cloud.Cloud_Tests_Setup.Temporary_Directory
from enso_dev.Base_Tests.Network.Enso_Cloud.Audit_Log_Spec import Audit_Log_Event, get_audit_log_events

import project.Database.Postgres_Spec.Temporary_Data_Link_File
from project.Database.Postgres_Spec import get_configured_connection_details
from project.Util import all

polyglot java import java.lang.Thread

## Tests the audit capabilities of the connection.
   It takes a data link to the connection. This data link is expected to be a local data link (from the filesystem), not a Cloud one.
add_specs suite_builder prefix ~datalink_to_connection database_pending =
    ## By default, these tests are run only on the Cloud mock, not on the real deployment.
       This is mostly because we don't yet have log filtering so the results on the real deployment could be massive.
       The local environment is more predictable for running these tests.
       The following flag can be changed to `False` to run it on the real cloud (if it is set up in the test context).
       This can be used to verify that the mock logic is consistent with the real thing.
       TODO Once https://github.com/enso-org/enso/issues/10919 is implemented, we can remove this flag.
    always_run_on_mock = True
    cloud_setup = if always_run_on_mock then Cloud_Tests_Setup.prepare_mock_setup else Cloud_Tests_Setup.prepare
    suite_builder.group prefix+"Audit Logs" pending=(cloud_setup.pending.if_nothing database_pending) group_builder->
        group_builder.specify "should see Database operations performed on a table through the standard workflow" <| cloud_setup.with_prepared_environment <|
            audited_connection = datalink_to_connection.read
            table_name = "audited-table-"+Random.uuid
            mem_table = Table.new [["X", [1, 2]], ["Y", ["my_payload", "foo"]]]
            audited_table = mem_table.select_into_database_table audited_connection table_name temporary=True . should_succeed
            materialized = audited_table.read
            materialized.column_names . should_equal ["X", "Y"]
            materialized.at "X" . to_vector . should_equal_ignoring_order [1, 2]
            materialized.at "Y" . to_vector . should_equal_ignoring_order ["my_payload", "foo"]
            audited_connection.drop_table audited_table.name . should_succeed

            # Retrying is needed as there may be some delay before the background thread finishes processing the logs.
            Test.with_retries <|
                all_events = get_audit_log_events
                relevant_events = all_events.filter e-> e.message.contains table_name
                Test.with_clue ((relevant_events.map .to_text).join '\n' 'Found relevant events are:\n' '\n') <|
                    create = relevant_events.find (e-> e.message.contains "CREATE")
                    create.should_succeed
                    create.user_email . should_equal Enso_User.current.email
                    create.metadata.get "connectionUri" . should_contain "jdbc:"
                    create.metadata.get "projectName" . should_equal enso_project.namespace+"."+enso_project.name

                    insert = relevant_events.find (e-> e.message.contains "INSERT INTO")
                    insert.should_succeed
                    # The insert query should not contain column cell data - only column names / metadata.
                    insert.message.should_not_contain "my_payload"

                    create_sequence_number = create.metadata.get "sequenceNumber"
                    insert_sequence_number = insert.metadata.get "sequenceNumber"
                    create_sequence_number.should_be_a Integer
                    insert_sequence_number.should_be_a Integer
                    (create_sequence_number < insert_sequence_number) . should_be_true

                    relevant_events.find (e-> e.message.contains "SELECT") . should_succeed
                    relevant_events.find (e-> e.message.contains "DROP") . should_succeed

        group_builder.specify "should see Database operations performed manually" <| cloud_setup.with_prepared_environment <|
            audited_connection = datalink_to_connection.read
            query = "SELECT 1 AS A, 2 AS B, "+(Random.integer 0 1000000).to_text+" AS C"
            t = audited_connection.read (SQL_Query.Raw_SQL query)
            # Force the connector to perform the query:
            t.at 0 . to_vector . should_equal [1]

            # Retrying is needed as there may be some delay before the background thread finishes processing the logs.
            Test.with_retries <|
                all_events = get_audit_log_events
                ## This is a bit white-box test - we assume the input query is found inside of the query that is run unchanged.
                   Currently that is OK, but if that ever stops being enough the test may need to be amended to be 'smarter'.
                relevant_event = all_events.find e-> e.message.contains query
                relevant_event.should_succeed

        group_builder.specify "should still be able to open a local data link if not logged in" <|
            # The logs should be sent to the local logger, but we cannot easily check that in tests.
            non_existent_file = (enso_project.data / "nonexistent-credentials-file")
            non_existent_file.exists.should_be_false
            Cloud_Tests_Setup.run_with_overridden_credentials non_existent_file <|
                locally_audited_connection = datalink_to_connection.read

                # We just check that we can read queries through this connection:
                locally_audited_connection.read (SQL_Query.Raw_SQL "SELECT 1 AS foo") . at 0 . to_vector . should_equal [1]

        # This test may only run on real cloud because the mock does not support creating datalinks.
        # Once the tests above can be run on real cloud too (#10919), we can merge all 3 cloud setups into a single one.
        real_cloud = Cloud_Tests_Setup.prepare
        test_root = Temporary_Directory.make "Audit-Logs-Datalinks"
        group_builder.specify "should know the asset id of the data link used for the connection" pending=real_cloud.real_cloud_pending <| real_cloud.with_prepared_environment <|
            # Upload our local reference data link to the cloud
            cloud_data_link = test_root.get / "audited-db.datalink"
            Data_Link.copy datalink_to_connection cloud_data_link . should_succeed

            # Set-up an audited connection through the Cloud data link
            audited_connection = cloud_data_link.read

            # Until https://github.com/enso-org/enso/issues/10919 is implemented, we switch over to cloud mock to force the audit logs to be sent there:
            mock_setup = Cloud_Tests_Setup.prepare_mock_setup
            mock_setup.with_prepared_environment <|
                table_name = "audited-table-"+Random.uuid
                mem_table = Table.new [["X", [1, 2]], ["Y", ["my_payload", "foo"]]]
                mem_table.select_into_database_table audited_connection table_name temporary=True . should_succeed

                Test.with_retries <|
                    all_events = get_audit_log_events
                    relevant_events = all_events.filter e-> e.message.contains table_name
                    Test.with_clue ((relevant_events.map .to_text).join '\n' 'Found relevant events are:\n' '\n') <|
                        create = relevant_events.find (e-> e.message.contains "CREATE")
                        create.should_succeed

                        create.metadata.get "connectionUri" . should_contain "jdbc:"
                        # Verify that it contains an asset id field:
                        asset_id = create.metadata.get "dataLinkAssetId"
                        # We cannot really assume too much about the ID other than that it is a string:
                        asset_id.should_be_a Text

main filter=Nothing =
    connection_details = get_configured_connection_details
    pending = if connection_details.is_nothing then "PostgreSQL test database is not configured. See README.md for instructions."
    data_link_file = Temporary_Data_Link_File.make connection_details
    suite = Test.build suite_builder->
        add_specs suite_builder "[PostgreSQL] " data_link_file.get database_pending=pending
    suite.run_with_filter filter
